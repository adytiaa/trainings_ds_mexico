{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark\n",
    "\n",
    "## Prerequisites\n",
    "* pyspark installed in jupyter kernel (e.g. with *conda create -n pyspark_env pyspark ipykernel* to create an environment with pyspark installed that you can select as a kernel in a jupyter notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "* What is Apache Spark?\n",
    "* Architecture\n",
    "* Spark Ecosystem\n",
    "* Resilient Distributed Dataset (RDD)\n",
    "* Introduction to SparkSQL and DataFrames:\n",
    "    + Creating a Spark Instance\n",
    "    + Reading & Writing Data\n",
    "    + The DataFrames/Datasets API\n",
    "    + Spark SQL\n",
    "    + Saving to a Persistent Table\n",
    "    + Bucketing, Sorting and partitioning\n",
    "    + Caching and caching storage levels\n",
    "* Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Apache Spark?\n",
    "\n",
    "[Spark](http://spark.apache.org/) is a general-purpose, distributed programming framework that was developed at the AMPLab at the University of California, Berkeley. It is open source software that provides an in-memory computation framework and it is also good for batch processing. Spark works well with real-time (or, better to say, near-real-time) data. It allows you to apply machine learning algorithms on semi-structured, structured, and streaming data.\n",
    "\n",
    "According to its research paper, it is approximately 100 times faster than its peer, Hadoop, because data can be cached in memory and many machine learning and graph algorithms are iteative. Caching intermediate data in iterative algorithms provides faster processing speed. Spark can be programmed with Java, Scala, Python, and R. In addition, Spark supports multiple data sources such as Parquet, JSON, Hive, Cassandra, CSV, text files and RDBMS tables.\n",
    "\n",
    "Spark might be considered as an improved [Hadoop](https://hadoop.apache.org/) because it uses the benefits of HDFS: reading data from and writing data to HDFS, and it is based on the [MapReduce](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html) algorithm. In addition, Spark handles iterative computation efficiently because data can be persisted in memory, and Spark provides APIs for machine learning, graph processing, streaming in different programming languages.\n",
    "\n",
    "## Advantages of Spark\n",
    "1. **Swift Processing:** Spark reduces the number of read-write to disk.\n",
    "\n",
    "2. **Dynamic in Nature:** Spark provides 80 high-level operators, which can help to develop a parallel executed application. For transformations, Spark adds them to a DAG (Directed Acyclic Graph) of computation and only when the driver requests data, this DAG is executed.\n",
    "\n",
    "3. **In-Memory Computation:** Data is cached so we do not have to read and write data to disk every time we access it (better performance).\n",
    "\n",
    "4. **Reusability:** The Spark code can e.g. be reused for batch-processing.\n",
    "\n",
    "5. **Fault Tolerance:** Through RDD, Spark provides fault tolerance. Spark RDDs are designed to handle the failure of any worker node in the cluster, which ensures that the loss of data is reduced to zero.\n",
    "\n",
    "https://data-flair.training/blogs/apache-spark-features/\n",
    "\n",
    "<center> Logistic Regression </center>\n",
    "![logistic-regression](https://user-images.githubusercontent.com/9319823/46016970-9fb87380-c0d6-11e8-86e0-7123a95c0309.png)\n",
    "\n",
    "\n",
    "## Disadvantages of Spark\n",
    "1. **Expensive:** In-memory capability can become a bottleneck when we want cost-efficient processing of big data as keeping data in memory is quite expensive.\n",
    "\n",
    "2. **Latency:** Apache Spark has a higher latency as compared to [Apache Flink](https://flink.apache.org/).\n",
    "\n",
    "3. **Manual Optimization:** The Spark job requires to be manually optimized and is adequate to specific datasets.\n",
    "\n",
    "4. **No File Management:** Apache Spark does not have its own file management system, thus it relies on other platforms like Hadoop.\n",
    "\n",
    "5. **Problem with Small Files:** If we use Spark with HDFS, we come across the small files issue. HDFS prefers a limited number of large files rather than a large number of small files. If you store your data zipped in S3 a similar issue arises as Spark has to have all these small zipped files at one core when we want to uncompress it.\n",
    "\n",
    "https://data-flair.training/blogs/limitations-of-apache-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark_architecture](https://user-images.githubusercontent.com/9319823/45994904-09645d80-c096-11e8-87e4-2b53f058ba99.png)\n",
    "\n",
    "The main components of the Spark architecture are the driver and the executors. For each PySpark application, there will be one driver program and one or more executors running on the cluster slave machines. Therefore, Spark follows a master/slave architecture.\n",
    "\n",
    "## Driver process / Master (Master Daemon)\n",
    "The driver is the process that coordinates with many executors running on various slave machines.\n",
    "\n",
    "    - The ***SparkContext*** object is created by the driver, and it is the main entry point to a (Py)Spark application.\n",
    "    - The Spark driver also contains various components such as *DAGScheduler*, *TaskScheduler*, *BackendScheduler* and *BlockManager* which are responsible for the translation of Spark user code into actual Spark jobs executed on the cluster. \n",
    "\n",
    "\n",
    "## Executors / Slaves (Worker Daemon)\n",
    "Executors are slave processes. An executor runs tasks. It also has the capability to cache data in memory.\n",
    "\n",
    "    - An executor is a distributed agent responsible for the execution of tasks. Every Spark applications has its own executor process.\n",
    "    - They usually run for the entire lifetime of a Spark application and this phenomenon is known as **“Static Allocation of Executors”**.\n",
    "    - However, users can also opt for dynamic allocations of executors wherein they can add or remove Spark executors dynamically to match with the overall workload.\n",
    "    - Executor performs all the data processing.\n",
    "    - Reads from and writes data to external sources.\n",
    "    - Executors store the computation results data in-memory, cache or on hard disk drives.\n",
    "    - Interact with the storage systems.\n",
    "    \n",
    "    \n",
    "## Cluster Manager\n",
    "An external service responsible for acquiring resources on the Spark cluster and allocating them to a Spark job.\n",
    "    - Hadoop YARN and Apache Mesos are examples of cluster manager.\n",
    "    - Standalone mode - simple local Spark cluster manager.\n",
    "    - Choosing a cluster manager for any Spark application depends on the goals of the application because all cluster managers provide different set of scheduling capabilities.\n",
    "\n",
    "The driver splits our application into small tasks; a task is the smallest unit of the application. Tasks are run on different executors in parallel, one task per partition. The driver is also responsible for scheduling tasks to different executors. Also, The **cluster manager** manages cluster resources. The driver talks to the cluster manager to negotiate resources. The cluster manager also schedules tasks on behalf of the driver on various slave executor processes.\n",
    "\n",
    "Spark is dispatched with the Standalone Cluster Manager. However, it can also be configured on [YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) and [Apache Mesos](http://mesos.apache.org/). Spark can be also started in local mode (i.e. on a single machine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Ecosystem\n",
    "\n",
    "\n",
    "![spark-stack](https://user-images.githubusercontent.com/9319823/45998657-ca3d0900-c0a3-11e8-8bb8-32672e87d119.png)\n",
    "\n",
    "Spark ecosytem has five components: [Spark Core API](https://spark.apache.org/docs/1.6.0/index.html), [SQL and DataFrames](http://spark.apache.org/sql/), [MLlib](http://spark.apache.org/mllib/) for machine learning, [GraphX](http://spark.apache.org/graphx/), and [Spark Streaming](http://spark.apache.org/streaming/). You can combine these libraries seamlessly in the same application.\n",
    "\n",
    "## Spark Core\n",
    "\n",
    "All the functionalities provided by Apache Spark are built on the top of Spark Core. It provides the in-memory computation capability. Thus Spark Core is the foundation of parallel and distributed processing of huge dataset.\n",
    "\n",
    "Spark Core is embedded with a special collection called **RDD** (resilient distributed dataset). RDD is among the abstractions of Spark. **Spark RDD handles partitioning data across all the nodes in a cluster**. It holds them in the memory pool of the cluster as a single unit. There are two operations performed on RDDs: Transformations and Actions.\n",
    "   - **Transformations:** are functions that produce a new RDD from the existing RDDs.\n",
    "   - **Actions:** return a value to the driver program.\n",
    "\n",
    "Operations are **evaluated lazily**: the execution will not start until an action is triggered. This increases manageability, saves computation and thus increases optimization and performance. The **transformations are stored as directed acyclic graphs (DAG)**. So, every action on the RDD will make Apache Spark recompute the DAG.\n",
    "\n",
    "Apache Spark supports two types of partitioning: **Hash Partitioning** and **Range Partitioning**. The partitioning technique should be based on the available resources, external data sources and transformations used to derive the RDD.\n",
    "\n",
    "Basics of **partitioning**:\n",
    "- Every node in a Spark cluster contains one or more partition(s).\n",
    "- Partitions in Spark do not span multiple machines.\n",
    "- Tuples in the same partition are guaranteed to be on the same machine.\n",
    "- Spark assigns one task per partition and each worker can process one task at a time.\n",
    "- The number of partitions used in Spark is configurable and having too few (causing less concurrency, data skewing & improper resource utilization) or too many (causing task scheduling to take more time than the actual execution time) partitions is not good. By default, it is set to the total number of cores on all the executor nodes.\n",
    "\n",
    "https://dzone.com/articles/an-intro-to-apache-spark-partitioning-what-you-nee\n",
    "\n",
    "**Key features of Spark Core are:** \n",
    "* essential I/O functionalities\n",
    "* task dispatching\n",
    "* fault recovery\n",
    "* significant in programming and observing the role of the Spark cluster.\n",
    "\n",
    "![pg57h](https://user-images.githubusercontent.com/9319823/46287815-10063f80-c584-11e8-9f03-88f5d7d033d5.png)\n",
    "\n",
    "## Spark SQL\n",
    "\n",
    "The Spark SQL module allows SQL-like analysis on a huge amount of structured or semi-structured data. Spark SQL can be connected to Apache Hive. Spark SQL introduced the DataFrame, which is a tabular representation of structured data, similar to a table in a relational database management system.\n",
    "\n",
    "Spark SQL is a distributed framework for structured data processing. Using Spark SQL, **Spark gets more information about the structure of data and the computation**. With this information, Spark can perform additional optimization. It uses the same execution engine while computing an output. It **does not depend on the API/ language to express the computation**.\n",
    "\n",
    "It also enables powerful, interactive, analytical applications across both streaming and historical data. Spark SQL is the Spark module for structured data processing. Thus, it acts as a distributed SQL query engine.\n",
    "\n",
    "**Key features of Spark SQL include:** \n",
    "* Spark integration\n",
    "* Uniform data access\n",
    "* Performance and Scalability\n",
    "* Full compatibility Hive\n",
    "* Standard Connectivity\n",
    "\n",
    "https://spark.apache.org/sql/\n",
    "\n",
    "https://www.edureka.co/blog/spark-sql-tutorial/\n",
    "\n",
    "## Spark ML\n",
    "\n",
    "The MLlib library offers scalable and easy-to-use machine-learning algorithms. MLlib supports many machine-learning algorithms for classification, clustering, text analysis, and more. Also, some lower level machine learning primitives like generic gradient descent optimization algorithm are available in MLlib.\n",
    "\n",
    "In Spark version 2.0, the DataFrame-based API is the primary Machine Learning API for Spark. So, from now on MLlib will not add any new feature to the RDD-based API. The reason behind this is that DataFrames are more user-friendly than RDDs. Some of the benefits of using DataFrames are:\n",
    "* the usage of Spark Datasources\n",
    "* SQL DataFrame queries use Tungsten and Catalyst optimizations\n",
    "* uniform APIs across languages. \n",
    "\n",
    "MLlib also uses the **linear algebra package Breeze**. Breeze is a collection of libraries for numerical computing and machine learning.\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "### GraphX\n",
    "\n",
    "GraphX is Apache Spark's API for graphs and graph-parallel computation. It unifies ETL, exploratory analysis, and iterative graph computation within a single system. You can view the same data as both graphs and collections, transform and join graphs with RDDs efficiently, and write custom iterative graph algorithms using the Pregel API.\n",
    "\n",
    "**Clustering, classification, traversal, searching, and pathfinding** is also possible in graphs. Furthermore, GraphX extends Spark RDD by bringing in light a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge. GraphX also optimizes the way in which we can represent vertex and edges when they are primitive data types. To support graph computation it supports fundamental operators (e.g., subgraph, join Vertices, and aggregate Messages) as well as an optimized variant of the Pregel API.\n",
    "\n",
    "https://spark.apache.org/graphx/\n",
    "\n",
    "## Spark Streaming\n",
    "\n",
    "Spark Streaming brings Apache Spark's language-integrated API to stream processing, letting you write streaming jobs the same way you write batch jobs. It supports Java, Scala and Python. It is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.\n",
    "\n",
    "Spark can access data from sources like Kafka, Flume, Kinesis or TCP socket. The processed data is pushed to the file system, databases and live dashboards. Spark uses micro-batching for (near) real-time streaming. **Micro-batching** is a technique that allows a process or task to treat a stream as a sequence of small batches of data. Hence, Spark Streaming groups the live data into small batches. \n",
    "\n",
    "Spark Streaming works in three phases: **(1) gathering**, **(2) processing**, and **(3) data storage**.\n",
    "  1. It provides two categories of built-in streaming sources: \n",
    "      - **Basic sources:** file systems and socket connections \n",
    "      - **Advanced sources:** sources like Kafka, Flume, Kinesis, etc.\n",
    "  2. The gathered data is processed using complex algorithms expressed with a high-level function.\n",
    "  3. The processed data is pushed out to file systems, databases, and live dashboards.\n",
    "\n",
    "A **DStream** in Spark is a continuous stream of data. We can form a DStream in two ways: from sources such as Kafka, Flume, and Kinesis or by high-level operations on other DStreams. Thus, DStream is internally a sequence of RDDs.\n",
    "\n",
    "![streaming-arch](https://user-images.githubusercontent.com/9319823/45999822-150c5000-c0a7-11e8-8a8a-f88b2c5b1c88.png)\n",
    "\n",
    "https://spark.apache.org/streaming/\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD (Resilient Distributed Dataset)\n",
    "\n",
    "A Resilient Distributed Datasets (RDD) is the basic abstraction in Spark. It represents an **immutable, partitioned collection of elements** that can be operated on in parallel. This class contains the basic operations available on all RDDs, such as *map*, *filter*, and *persist*.\n",
    "\n",
    "RDDs are a collection of various data that are so big in size, that they cannot fit into a single node and should be partitioned across various nodes. As a quick reminder from the last section, Apache Spark automatically partitions RDDs and distributes the partitions across different nodes.\n",
    "\n",
    "https://spark.apache.org/docs/2.1.0/programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('myApp')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating RDDs**\n",
    "\n",
    "* The key point to note in parallelized collection is the number of partition the dataset is cut into. Spark will run **one task for each partition of the cluster**. Usually, there are **two to four partitions for each CPU in the cluster**. Spark sets number of partition based on our cluster.\n",
    "* It is possible to read files from different sources such as local file system, HDFS, Cassandra, HBase, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - PARALLELIZED COLLECTION\n",
    "temperature = sc.parallelize((28.9, 30.6, 25.0, 29.1, 32.3, 31.0))\n",
    "\n",
    "# 2 - EXTERNAL DATASETS\n",
    "text = sc.textFile(\"data/simple_text.txt\")\n",
    "\n",
    "# 3 - FROM ANOTHER RDD\n",
    "high_temps = temperature.filter(lambda t: t >= 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Returning RDD data**\n",
    "\n",
    "* The action operation **collect()** should be used carefully since it returns all data. In this example, it returns the whole text as a list. Be careful, when you operate with large datasets!\n",
    "\n",
    "* The action operation **take(n)** returns *n* values/results. If you want to see all results, use **collect()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a simple', 'text file', 'with some text']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28.9, 30.6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering data**\n",
    "\n",
    "* **filter()** returns a new RDD, containing only the elements that meet the predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28.9, 25.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_temp = temperature.filter(lambda t: (t > 20) & (t < 29))\n",
    "range_temp.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of operations on RDD 01** Word count and save output into a file.\n",
    "\n",
    "* **flatMap()** takes a line from the input RDD, applies a function on that line, and returns a list of elements.\n",
    "\n",
    "* **map()** takes a line from the input RDD, applies a function on that line, and returns only one element.\n",
    "\n",
    "* **reduceByKey()**, in a dataset (K, V), the pairs on the same machine with the same key are combined, before the data is shuffled.\n",
    "\n",
    "* **saveAsTextFile()** saves the RDD object as a text file, using string representations of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/dan/git/u42/trainings/hadoop-spark/data/simple_word_count already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-46ca16599ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/simple_word_count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/dan/git/u42/trainings/hadoop-spark/data/simple_word_count already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "wc = text.flatMap(lambda x: x.split(\" \")).map(lambda a: (a,1)).reduceByKey(lambda a,b: a + b)\n",
    "wc.saveAsTextFile(\"data/simple_word_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark SQL and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark SQL is a Spark module for structured data processing**. It originated as Apache Hive to run on top of Spark and is now directly integrated with the Spark stack. Apache Hive had certain limitations such as no resume capability and bad performance in medium-to-big sized datasets. Spark SQL was built to overcome these drawbacks and replace Apache Hive.\n",
    "\n",
    "Unlike the basic Spark RDD API, the Spark SQL interface provides more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform additional optimizations.\n",
    "\n",
    "Spark SQL is organized into four libraries as follows:\n",
    "\n",
    "|           Libraries           |                            Description                          |\n",
    "| :---------------------------- | :-------------------------------------------------------------- |\n",
    "| Data Source API               | The universal API for loading and storing structured data       |\n",
    "| DataFrame API                 | The distributed collection of data organized into named columns |\n",
    "| SQL Interpreter And Optimizer | Based on the functional programming constructed in Scala        |\n",
    "| SQL Service                   | The entry point for working along structured data in Spark      |\n",
    "\n",
    "\n",
    "Important classes of Spark SQL (**pyspark.sql.*(class)***) and DataFrames are described in the following table:\n",
    "\n",
    "\n",
    "| \tClasses\t\t         | \t\t\t          Description\t\t\t\t                |\n",
    "| :--------------------- | :----------------------------------------------------------- |\n",
    "| SparkSession           | Main entry point for **DataFrame** and SQL functionality\t    |\n",
    "| DataFrame              | A distributed collection of data grouped into named columns\t|\n",
    "| Column                 | A column expression in a **DataFrame**\t\t\t            |\n",
    "| Row                    | A row of data in a DataFrame\t\t\t\t\t                |\n",
    "| GroupedData            | Aggregation methods, returned by **DataFrame.groupBy()**\t    |\n",
    "| DataFrameNaFunctions   | Methods for handling missing data (null values)\t \t        |\n",
    "| DataFrameStatFunctions | Methods for statistics functionality\t\t\t\t            |\n",
    "| Functions              | List of built-in functions available for **DataFrame**\t    |\n",
    "| Types                  | List of data types available\t\t\t\t\t                |\n",
    "| Window                 | For working with window functions\t\t\t\t            |\n",
    "\n",
    "\n",
    "## Hive Integration\n",
    "One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. Spark SQL supports the HiveQL syntax as well as **Hive SerDes** and **UDFs**, allowing you to access existing Hive warehouses.\n",
    "\n",
    "![sql-hive-arch](https://user-images.githubusercontent.com/9319823/47175460-71b00300-d313-11e8-9a09-5e8f3bdb97dc.png)\n",
    "\n",
    "\n",
    "| Library   |     \t\t\t    Description \t\t\t                              |\n",
    "| :-------- | :------------------------------------------------------------------------ |\n",
    "| Metastore | Stores metadata such as the schema and location for each of the tables   |\n",
    "| HiveQL    | Hive Query Language based on SQL (does not strictly follow the full *SQL-92* standard) |\n",
    "| UDFs      | Defines new column-based functions that extend the vocabulary of Spark SQL’s DSL* for transforming Datasets. UDFs are black boxes in their execution |\n",
    "| SerDes    | Handles serialization and deserialization (I/O) and also interpreting the results of serialization as individual fields for processing |\n",
    "\n",
    "*DSL = domain-specific language \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dan-ThinkPad-T470p.berlin.artnology.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fab344d2d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"learnig_pyspark\") \\\n",
    "                    .config(conf=SparkConf()).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading & Writing Data\n",
    "\n",
    "- Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view.\n",
    "- You can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source.\n",
    "- DataFrames loaded from any data source type can be converted into other types.\n",
    "- Registering a DataFrame as a temporary view allows you to run SQL queries over its data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from **CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, job: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data = spark.read.csv(\"data/people.csv\", header=\"true\", sep=\";\", inferSchema=\"true\")\n",
    "csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd option to load data from a source\n",
    "```python\n",
    "csv_data = spark.read.format(\"com.databricks.spark.csv\")\n",
    "                     .options(header=\"true\", sep=\";\", inferSchema=\"true\")\n",
    "                     .load(\"data/people.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from **CSV file** directly from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url_file = \"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/\" \\\n",
    "            \"raw/d546eaee765268bf2f487608c537c05e22e4b221/iris.csv\"\n",
    "\n",
    "iris = spark.createDataFrame(pd.read_csv(url_file))\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Michael: string,  29: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_data = spark.read.format(\"com.databricks.spark.csv\")\\\n",
    "                     .options(header=\"true\", sep=\",\", inferSchema=\"true\").load(\"data/people.txt\")\n",
    "txt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from **JSON file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = spark.read.json(\"data/people.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Spark can read other formats such as `text`, `avro`, and `parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing** into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data.write.format(\"parquet\").mode('overwrite').save(\"data/people_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2nd option to save dataframe into a file\n",
    "```python\n",
    "csv_data.write.save(\"data/people_parquet\", format=\"parquet\", mode=\"{overwrite|append|ignore}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Save Mode | When saving a DataFrame to a data source, if data already exists, ... |\n",
    "|:--------- |:--------------------------------------------------------------------- |\n",
    "| Error     | an exception is thrown (default)                                      |\n",
    "| Append    | contens of the DataFrame is appended to existing data                 |\n",
    "| Overwrite | existing data is overwritten by the content of the DataFrame          |\n",
    "| Ignore    | content is not saved and existing data is not changed                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating **temporary view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data.createOrReplaceTempView(\"people_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DataFrames/Datasets API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a **DataFrame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Row:** can be used to create a row object by using named arguments, the fields will be sorted by names. It is not allowed to omit a named argument to represent the value is None or missing. This should be explicitly set to None in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----+-------+\n",
      "|age|identification|name|surname|\n",
      "+---+--------------+----+-------+\n",
      "| 29|             1|John|    Doe|\n",
      "| 25|             2|Jane|    Doe|\n",
      "+---+--------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "r = [Row(identification=1,name='John',surname='Doe',age=29),\n",
    "     Row(identification=2,name='Jane',surname='Doe',age=25)]\n",
    "df_data = spark.createDataFrame(r)\n",
    "\n",
    "df_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **show(n)** presents the first *n* elements of the DataFrame. The default value for *n* is 10. \n",
    "\n",
    "> **head(n)** acts similar to *show(n)* by showing the first *n* observations, but returns a list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=29, identification=1, name='John', surname='Doe'),\n",
       " Row(age=25, identification=2, name='Jane', surname='Doe')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counting** the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the **columns** and **count** the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'identification', 'name', 'surname']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"number of columns: {0}\".format(len(df_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a **DataFrame** with a complex structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|       department|           employees|\n",
      "+-----------------+--------------------+\n",
      "|[1, Data Science]|[[John, Doe, john...|\n",
      "|   [2, Marketing]|[[Helena, Ster, h...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept1 = Row(id=1, name=\"Data Science\")\n",
    "dept2 = Row(id=2, name=\"Marketing\")\n",
    "\n",
    "Employee = Row(\"firstname\",\"lastname\",\"email\",\"salary\")\n",
    "e1 = Employee(\"John\",\"Doe\",\"johnd@domain.de\",90000)\n",
    "e2 = Employee(\"Jone\",\"Doe\",\"janed@domain.de\",92000)\n",
    "e3 = Employee(\"Helena\",\"Ster\",\"he.st@domain.de\",100000)\n",
    "\n",
    "dwe1 = Row(department=dept1, employees=[e1,e2])\n",
    "dwe2 = Row(department=dept2, employees=[e3])\n",
    "\n",
    "dwe_seq = [dwe1, dwe2]\n",
    "dwe_df = spark.createDataFrame(dwe_seq)\n",
    "\n",
    "dwe_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking **DataFrame** schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- employees: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- firstname: string (nullable = true)\n",
      " |    |    |-- lastname: string (nullable = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dwe_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inferring** the schema using reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"data/people.txt\")\n",
    "slines = lines.map(lambda x: x.split(\",\"))\n",
    "parts = slines.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "\n",
    "tdf = spark.createDataFrame(parts)\n",
    "tdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmatically **specifying** the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, StructField, StructType\n",
    "\n",
    "def set_dta_type(stp):\n",
    "    if stp == \"int\":\n",
    "        return IntegerType()\n",
    "    elif stp == \"float\":\n",
    "        return FloatType()\n",
    "    elif stp == \"double\":\n",
    "        return DoubleType()\n",
    "    else:\n",
    "        return StringType() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"data/people.txt\")\n",
    "slines = lines.map(lambda x: x.split(\",\"))\n",
    "parts = slines.map(lambda x: (x[0], int(x[1].strip())))\n",
    "\n",
    "schemaNames = \"name age\"\n",
    "schemaTypes = \"string int\"\n",
    "\n",
    "fields = [StructField(fn, set_dta_type(ft)) for fn, ft in zip(schemaNames.split(), schemaTypes.split())]\n",
    "schema = StructType(fields)\n",
    "\n",
    "df_people = spark.createDataFrame(parts, schema)\n",
    "df_people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Michael', age=29),\n",
       " Row(name='Andy', age=30),\n",
       " Row(name='Justin', age=19)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Datasets** were introduced in 2015 as part of the Apache Spark 1.6 release. The goal for datasets was to provide a type-safe, programming interface. This allowed developers to work with semi-structured data (like JSON or key-value pairs) with compile time type safety (that is, production applications can be checked for errors before they run). Part of the reason why Python does not implement a Dataset API is because Python is not a type-safe language.\n",
    "\n",
    "> As **Dataset** is strongly typed API and Python is dynamically typed, runtime objects (values) have a type, as opposed to static typing where variables have a type. Therefore, there is no native support for the Dataset API in Pyspark. Only Scala and Java offer support for Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select** columns from DataFrame\n",
    "\n",
    "To subset the columns, we need to use the **select** operation on the DataFrame and we need to pass the column names separated by commas inside the select operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pyspark.sql.functions** is a collections of builtin functions such as **cols**, **asin** and **avg**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------+\n",
      "|sepal_length|petal_length|species|\n",
      "+------------+------------+-------+\n",
      "|         5.1|         1.4| setosa|\n",
      "|         4.9|         1.4| setosa|\n",
      "|         4.7|         1.3| setosa|\n",
      "|         4.6|         1.5| setosa|\n",
      "|         5.0|         1.4| setosa|\n",
      "+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(\"sepal_length\",\"petal_length\",\"species\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a **set of features** from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = iris.columns\n",
    "to_del = [\"species\", \"sepal_width\"] # subset of features to be removed\n",
    "dcols = list(set(cols) - set(to_del))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+\n",
      "|petal_width|sepal_length|petal_length|\n",
      "+-----------+------------+------------+\n",
      "|        0.2|         5.1|         1.4|\n",
      "|        0.2|         4.9|         1.4|\n",
      "|        0.2|         4.7|         1.3|\n",
      "|        0.2|         4.6|         1.5|\n",
      "|        0.2|         5.0|         1.4|\n",
      "+-----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(dcols).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------------------------+\n",
      "|sepal_length|CASE WHEN (petal_length < 2.1) THEN 1 ELSE 0 END|\n",
      "+------------+------------------------------------------------+\n",
      "|         5.1|                                               1|\n",
      "|         4.9|                                               1|\n",
      "|         4.7|                                               1|\n",
      "|         4.6|                                               1|\n",
      "|         5.0|                                               1|\n",
      "+------------+------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(\"sepal_length\", F.when(F.col(\"petal_length\") < 2.1, 1).otherwise(0)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [When](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=sort#pyspark.sql.functions.when) function evaluates a list of conditions and returns one of multiple possible result expressions. If **Column*.otherwise()*** is not invoked, None is returned for unmatched conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an **alias** for a columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|sepal_length|pl_less_thr|\n",
      "+------------+-----------+\n",
      "|         5.1|          1|\n",
      "|         4.9|          1|\n",
      "|         4.7|          1|\n",
      "|         4.6|          1|\n",
      "|         5.0|          1|\n",
      "+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(\"sepal_length\", F.when(F.col(\"petal_length\") < 2.1, 1).otherwise(0)\\\n",
    "                             .alias(\"pl_less_thr\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering** the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|sepal_length|\n",
      "+------------+\n",
      "|         5.1|\n",
      "|         4.9|\n",
      "|         4.7|\n",
      "|         4.6|\n",
      "|         5.0|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.filter(F.col(\"petal_length\") < 2.1).select(\"sepal_length\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass the feature name inside square brackets:\n",
    "```python\n",
    "iris.filter(iris[\"petal_length\"] < 2.1).select(\"sepal_length\").show(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting** the DataFrame by a column\n",
    "\n",
    "Sort ascending vs. descending. Specify list for multiple sort orders. If a list is specified, length of the list must equal length of the cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+---------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  species|\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "|         5.7|        4.4|         1.5|        0.4|   setosa|\n",
      "|         5.5|        4.2|         1.4|        0.2|   setosa|\n",
      "|         5.2|        4.1|         1.5|        0.1|   setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2|   setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4|   setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4|   setosa|\n",
      "|         7.9|        3.8|         6.4|        2.0|virginica|\n",
      "|         5.1|        3.8|         1.5|        0.3|   setosa|\n",
      "|         7.7|        3.8|         6.7|        2.2|virginica|\n",
      "|         5.7|        3.8|         1.7|        0.3|   setosa|\n",
      "|         5.1|        3.8|         1.9|        0.4|   setosa|\n",
      "|         5.1|        3.8|         1.6|        0.2|   setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2|   setosa|\n",
      "|         5.3|        3.7|         1.5|        0.2|   setosa|\n",
      "|         5.1|        3.7|         1.5|        0.4|   setosa|\n",
      "|         4.6|        3.6|         1.0|        0.2|   setosa|\n",
      "|         7.2|        3.6|         6.1|        2.5|virginica|\n",
      "|         5.0|        3.6|         1.4|        0.2|   setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3|   setosa|\n",
      "|         5.1|        3.5|         1.4|        0.2|   setosa|\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.sort(\"sepal_width\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option to sort:\n",
    "```python\n",
    "iris.sort(iris.sepal_width.desc()).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting** the DataFrame by passing more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+---------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  species|\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "|         5.7|        4.4|         1.5|        0.4|   setosa|\n",
      "|         5.5|        4.2|         1.4|        0.2|   setosa|\n",
      "|         5.2|        4.1|         1.5|        0.1|   setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2|   setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4|   setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4|   setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3|   setosa|\n",
      "|         5.1|        3.8|         1.6|        0.2|   setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3|   setosa|\n",
      "|         5.1|        3.8|         1.9|        0.4|   setosa|\n",
      "|         7.9|        3.8|         6.4|        2.0|virginica|\n",
      "|         7.7|        3.8|         6.7|        2.2|virginica|\n",
      "|         5.4|        3.7|         1.5|        0.2|   setosa|\n",
      "|         5.3|        3.7|         1.5|        0.2|   setosa|\n",
      "|         5.1|        3.7|         1.5|        0.4|   setosa|\n",
      "|         4.6|        3.6|         1.0|        0.2|   setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|   setosa|\n",
      "|         7.2|        3.6|         6.1|        2.5|virginica|\n",
      "|         5.0|        3.5|         1.3|        0.3|   setosa|\n",
      "|         5.5|        3.5|         1.3|        0.2|   setosa|\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.sort([\"sepal_width\",\"petal_length\"], ascending=[0,1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grouping** data by a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|   species|count|\n",
      "+----------+-----+\n",
      "| virginica|   50|\n",
      "|versicolor|   50|\n",
      "|    setosa|   50|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.groupBy(\"species\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grouping** data by a given column and perform the aggregation on another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|   species| avg_sepal_length|\n",
      "+----------+-----------------+\n",
      "| virginica|6.587999999999998|\n",
      "|versicolor|            5.936|\n",
      "|    setosa|5.005999999999999|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.groupBy(\"species\").agg(F.avg(\"sepal_length\").alias(\"avg_sepal_length\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking **missing** values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Simple count and compare*\n",
    "\n",
    "- It counts the total number of values that are not *null*. Therefore, you must compare it with the total number of rows to identify the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------+------------+-----------+-------+\n",
      "|summary|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+-------+------------+-----------+------------+-----------+-------+\n",
      "|  count|         150|        150|         150|        150|    150|\n",
      "+-------+------------+-----------+------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Count the missing values*\n",
    "\n",
    "- Create a function that receives a DataFrame and counts the missing values per column.\n",
    "- The output shows the number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing_values(df):\n",
    "    df.select([F.count(F.when(F.isnull(c),c)).alias(c) for c in df.columns]).show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Missing values on *Iris* DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|           0|          0|           0|          0|      0|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_missing_values(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Missing values on *People* DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|    USA|John|   89013|\n",
      "|Germany|Jane|    null|\n",
      "|   null|null|   10133|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create people DataFrame with some missing values\n",
    "r = [Row(name='John', country='USA', zip_code=89013),\n",
    "     Row(name='Jane', country='Germany', zip_code=None),\n",
    "     Row(name=None, country=None, zip_code=10133)]\n",
    "people = spark.createDataFrame(r)\n",
    "\n",
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|      1|   1|       1|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_missing_values(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dealing** with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fill in with a value*\n",
    "\n",
    "- If you pass a number, it will be replaced only in numerical features. The same applies for Categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|    USA|John|   89013|\n",
      "|Germany|Jane|   80000|\n",
      "|   null|null|   10133|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.na.fill(80000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A **subset** of feature might be also passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|    USA|John|   89013|\n",
      "|Germany|Jane|    null|\n",
      "|   null|Jack|   10133|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.na.fill(\"Jack\", subset=[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace value*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|    USA|John|   80000|\n",
      "|Germany|Jane|    null|\n",
      "|   null|null|   10133|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.na.replace(89013, 80000, subset=[\"zip_code\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Drop missing values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|    USA|John|   89013|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**drop() Parameters:**\n",
    "\n",
    "- **how**: ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "\n",
    "- **thresh** (int, default:None): If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "\n",
    "- **subset** (optional): List of column names to consider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|    USA|John|   89013|\n",
      "|Germany|Jane|    null|\n",
      "|   null|null|   10133|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|    USA|John|   89013|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|    USA|John|   89013|\n",
      "|Germany|Jane|    null|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop **duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   species|\n",
      "+----------+\n",
      "| virginica|\n",
      "|versicolor|\n",
      "|    setosa|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(\"species\").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **dropDuplicates** has also a parameter *subset* that informs a subset of features to be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "iris.dropDuplicates([\"species\"]).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding** a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|thr_sepal_length|\n",
      "+------------+-----------+------------+-----------+-------+----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|               1|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|               1|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|               0|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|               0|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|               1|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|               1|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|               0|\n",
      "+------------+-----------+------------+-----------+-------+----------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = iris.withColumn(\"thr_sepal_length\", F.when(F.col(\"sepal_length\") > 4.7,1).otherwise(0))\n",
    "iris.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updating** a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|thr_sepal_length|\n",
      "+------------+-----------+------------+-----------+------+----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|setosa|               1|\n",
      "|         4.9|        3.0|         1.4|        0.2|setosa|               1|\n",
      "|         4.7|        3.2|         1.3|        0.2|setosa|               0|\n",
      "|         4.6|        3.1|         1.5|        0.2|setosa|               0|\n",
      "|         5.0|        3.6|         1.4|        0.2|setosa|               1|\n",
      "+------------+-----------+------------+-----------+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.withColumnRenamed(\"species\", \"target\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing** a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = iris.drop(\"thr_sepal_length\")\n",
    "iris.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the **summary statistics**\n",
    "\n",
    "The **describe** operation is use to calculate the summary statistics of numerical column(s) in a DataFrame. If we do not specify the name of columns it will calculate summary statistics for all numerical columns present in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|summary|      sepal_length|        sepal_width|      petal_length|       petal_width|  species|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|  count|               150|                150|               150|               150|      150|\n",
      "|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|     null|\n",
      "| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|     null|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|   setosa|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|virginica|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **describe** operation works only for numerical features, that is, we must find another way to compute the frequency of **categorical features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the frequency of **categorical features** using **GroupBy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|   species|count|\n",
      "+----------+-----+\n",
      "| virginica|   50|\n",
      "|versicolor|   50|\n",
      "|    setosa|   50|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.groupBy(\"species\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running SQL **Queries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must register the DataFrame as a SQL temporary view in order to run SQL queries.\n",
    "\n",
    "**createOrReplaceTempView** creates a new temporary view using a SparkDataFrame in the Spark Session. If a temporary view with the same name already exists, it will be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.createOrReplaceTempView(\"iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of query 01 : All data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iris\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of query 02 : Where*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|\n",
      "|         4.6|        3.6|         1.0|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM iris\n",
    "WHERE sepal_length <= 4.6\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of query 03 : startswith / endswith*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------+-------+\n",
      "|sepal_length|petal_length|species|ca_eval|\n",
      "+------------+------------+-------+-------+\n",
      "|         5.1|         1.4| setosa|   true|\n",
      "|         4.9|         1.4| setosa|   true|\n",
      "|         4.7|         1.3| setosa|   true|\n",
      "|         4.6|         1.5| setosa|   true|\n",
      "|         5.0|         1.4| setosa|   true|\n",
      "+------------+------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(\"sepal_length\", \"petal_length\", \"species\",\n",
    "            F.col(\"species\").startswith(\"se\").alias(\"ca_eval\")).filter(F.col(\"ca_eval\") == \"true\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+-------+\n",
      "|sepal_length|petal_length|  species|ca_eval|\n",
      "+------------+------------+---------+-------+\n",
      "|         6.3|         6.0|virginica|   true|\n",
      "|         5.8|         5.1|virginica|   true|\n",
      "|         7.1|         5.9|virginica|   true|\n",
      "|         6.3|         5.6|virginica|   true|\n",
      "|         6.5|         5.8|virginica|   true|\n",
      "+------------+------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(\"sepal_length\", \"petal_length\", \"species\",\n",
    "            F.col(\"species\").endswith(\"ca\").alias(\"ca_eval\")).filter(F.col(\"ca_eval\") == \"true\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of query 04 : like*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+\n",
      "|name|country|name LIKE Jane|\n",
      "+----+-------+--------------+\n",
      "|John|    USA|         false|\n",
      "|Jane|Germany|          true|\n",
      "|null|   null|          null|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(\"name\",\"country\", people.name.like(\"Jane\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We could also use *filter* along with *like*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+\n",
      "|country|name|zip_code|\n",
      "+-------+----+--------+\n",
      "|Germany|Jane|    null|\n",
      "+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.filter(F.col(\"name\").like(\"Jane\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of query 05 : substring*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----+\n",
      "|sepal_length|petal_length|spec|\n",
      "+------------+------------+----+\n",
      "|         5.1|         1.4| set|\n",
      "|         4.9|         1.4| set|\n",
      "|         4.7|         1.3| set|\n",
      "|         4.6|         1.5| set|\n",
      "|         5.0|         1.4| set|\n",
      "+------------+------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(\"sepal_length\", \"petal_length\",\n",
    "            F.col(\"species\").substr(1,3).alias(\"spec\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of query 06 : between*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+---------+\n",
      "|sepal_length|petal_length|petal_width|pw_btween|\n",
      "+------------+------------+-----------+---------+\n",
      "|         5.1|         1.4|        0.2|    false|\n",
      "|         4.9|         1.4|        0.2|    false|\n",
      "|         4.7|         1.3|        0.2|    false|\n",
      "|         4.6|         1.5|        0.2|    false|\n",
      "|         5.0|         1.4|        0.2|    false|\n",
      "+------------+------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(\"sepal_length\", \"petal_length\", \"petal_width\",\n",
    "            F.col(\"petal_width\").between(1.2,1.9).alias(\"pw_btween\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SQL **on files** directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM parquet.`{0}`\n",
    "\"\"\".format(\"data/people_parquet\")\n",
    "\n",
    "users_df = spark.sql(query)\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to a Persistent Table\n",
    "\n",
    "- DataFrames can also be saved as persistent tables into Hive metastore using the *saveAsTable* command.\n",
    "- Spark will create a default local Hive metastore (using Derby) for you.\n",
    "- Unlike the createOrReplaceTempView command, saveAsTable will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore.\n",
    "- For file-based data sources, e.g. text, parquet, json, etc. you can specify a custom table path via the path option, e.g. df.write.option(\"path\", \"/some/path\").saveAsTable(\"t\"). When the table is dropped, the custom table path will not be removed and the table data is still there. If no custom table path is specified, Spark will write data to a default table path under the warehouse directory. When the table is dropped, the default table path will be removed too.\n",
    "- Starting from Spark 2.1, persistent datasource tables have per-partition metadata stored in the Hive metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.select(\"sepal_length\", \"petal_width\", \"species\").write.mode(\"overwrite\").saveAsTable(\"iris2\")\n",
    "# saved in the spark-warehouse folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing, Sorting and Partitioning\n",
    "\n",
    "- For file-based data sources, it is also possible to bucket and sort or partition the output. Bucketing and sorting are applicable only to persistent tables.\n",
    "- **partitionBy** creates a directory structure as described in the [Partition Discovery](https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#partition-discovery) section. Thus, it has limited applicability to columns with high cardinality. In contrast **bucketBy** distributes data across a fixed number of buckets and can be used when a number of unique values is unbounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people.write.bucketBy(42, \"name\").sortBy(\"Age\").saveAsTable(\"people_bucket\", mode=\"overwrite\")\n",
    "# creates people_bucket folder in spark-warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.write.partitionBy(\"job\").format(\"parquet\").save(\"job.parquet\", mode=\"overwrite\")\n",
    "# creates job.parquet folder in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df.write.partitionBy(\"job\").bucketBy(42, \"name\").saveAsTable(\"people_partitioned_bucketed\", mode=\"overwrite\")\n",
    "# creates people_partitioned_bucketed folder in spark-warehouse directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching and caching storage levels\n",
    "\n",
    "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like PageRank.\n",
    "\n",
    "- Proper caching is the key to high performance Spark.\n",
    "- Cache a DataFrame when it is used multiple times in the script.\n",
    "- The DataFrame is only cached after the first action such as *count()*.\n",
    "- Apache Spark will only cache the rows that are pulled by the action, this means that it will cache as many partitions as it has to read during the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking** if the DataFrame is cached\n",
    "\n",
    "- **StorageLevel** describes how an RDD/DataFrame is persisted.\n",
    "- The default storage level has changed to **MEMORY_AND_DISK** to match Scala in 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "StorageLevel(disk=false, memory=false, offheap=false, deserialized=false, replication=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StorageLevel() Parameters:**\n",
    "\n",
    "- **useMemory:** Boolean - use memory for data storage using *useMemory* flag. <br>\n",
    "- **useDisk:** Boolean - use disk for data storage using *useDisk* flag. <br>\n",
    "- **deserialized:** Boolean - store data in deserialized format using *deserialized* flag.<br>\n",
    "- **replication:** Int - replicate the data to other block managers using *replication* property.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Releasing memory** after using caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.unpersist()\n",
    "iris.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to *remove all cached tables* from the in-memory cache:\n",
    "\n",
    "```python\n",
    "sqlContext.clearCache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "**RDD**\n",
    "1. Create a **txt file** with the following line (heights): *1.79, 1.60, 1.89, 2.01, 2.32, 1.58, 1.47, 1.56*\n",
    "2. Create **load** the file into a RDD.\n",
    "3. Get the heights **above 2.0**.\n",
    "4. Save the output into a **txt file**.\n",
    "\n",
    "**DataFrame**\n",
    "1. Load the Auto MPG Data Set ([here](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)) into a DataFrame.\n",
    "2. Verify the number of rows and columns/features. \n",
    "3. Verify the data types, and if there is something wrong with them, please, fix it.\n",
    "4. Are there missing values?\n",
    "5. If yes, try to solve such a problem by filling up.\n",
    "4. Get a summary of the numerical values.\n",
    "\n",
    "## Challenge: MovieLens (1m)\n",
    "\n",
    "Download the [MovieLens](http://grouplens.org/datasets/movielens/) dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Which are the Top 10 best rated movies (with at least 10 total ratings)?\n",
    "2. Persons of which age group give the most ratings overall?\n",
    "3. Persons of which occupation give the best average ratings for comedies?\n",
    "4. Which single genre in average is the best rated by male persons?\n",
    "5. Based on the average rating: Would you say the common saying that females love romantic movies better than males is true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Kumar, R., 2018. ***PySpark Recipes***. Apress.\n",
    "2. Tomasz, D., 2017. ***Learning PySpark***. Packt Publishing.\n",
    "3. Spark.apache.org. (2018). ***Apache Spark™ - Unified Analytics Engine for Big Data***. [online] Available at: http://spark.apache.org/ [Accessed 25 Sep. 2018].\n",
    "4. Spark.apache.org. (2018). ***GraphX | Apache Spark***. [online] Available at: http://spark.apache.org/graphx/ [Accessed 25 Sep. 2018].\n",
    "5. Spark.apache.org. (2018). ***Spark Streaming - Spark 2.3.1 Documentation***. [online] Available at: https://spark.apache.org/docs/latest/streaming-programming-guide.html [Accessed 25 Sep. 2018].\n",
    "6. DeZyre. (2018). **Apache Spark Architecture Explained in Detail**. [online] Available at: https://www.dezyre.com/article/apache-spark-architecture-explained-in-detail/338 [Accessed 26 Sep. 2018].\n",
    "7. Data-flair.training. (2018). Apache Spark Ecosystem – Complete Spark Components Guide – DataFlair. [online] Available at: https://data-flair.training/blogs/apache-spark-ecosystem-components/ [Accessed 1 Oct. 2018].\n",
    "8. Laskowski, J. (2018). ***StorageLevel · Mastering Apache Spark***. [online] Jaceklaskowski.gitbooks.io. Available at: https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-StorageLevel.html [Accessed 29 Oct. 2018]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
