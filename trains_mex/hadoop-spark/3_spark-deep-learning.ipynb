{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Spark\n",
    "Databricks provides a Deep Learning Spark library to run Deep Learning models by leveraging built in models provided by Spark ML. It uses the fast distributed processing engine of Spark to run parallel and distributed models.\n",
    "\n",
    "https://github.com/databricks/spark-deep-learning\n",
    "\n",
    "# Installation\n",
    "We can either create a conda environment or start the deep learning library directly in the PySpark shell.\n",
    "\n",
    "## Conda Environment\n",
    "If you want to use the deep learning package from within your jupyter notebook, you can create a conda environment and use this environment as a kernel. Therefore, you have to create a new conda environment with the required packages. We will call that environment pyspark-dl:\n",
    "\n",
    "*conda create -n pyspark-dl python=3.6 pyspark six=1.11.0 nomkl pandas=0.23.4 h5py=2.8.0 pillow=4.1.1 cloudpickle=0.5.2 tensorflow=1.12.0 keras=2.2.4 paramiko=2.4.1 wrapt=1.10.11 nb_conda*\n",
    "\n",
    "Now you have to activate your *pyspark-dl* environment so we can install more required packages with pip:\n",
    "\n",
    "*source activate pyspark-dl*\n",
    "\n",
    "Run the following line to install the missing packages via pip:\n",
    "\n",
    "*pip install tensorframes kafka tensorflowonspark jieba*\n",
    "\n",
    "Now you can deactivate your environment again using the following command:\n",
    "\n",
    "*source deactivate pyspark-dl*\n",
    "\n",
    "Start jupyter notebook and select the *pyspark-dl* environment as a kernel (Kernel - Change Kernel - *pyspark-dl* or select the *pyspark-dl* kernel when creating a new notebook).\n",
    "\n",
    "\n",
    "See https://github.com/databricks/spark-deep-learning/blob/master/environment.yml for the full list of packages.\n",
    "\n",
    "## PySpark Shell\n",
    "\n",
    "In order to run PySpark with Deep Learning support in the shell, we need to invoke PySpark with Spark's Deep Learning library. We can use the following command to start a PySpark shell and install the packages of Spark Deep Learning:\n",
    "\n",
    "pyspark  --packages databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11\n",
    "\n",
    "Also see https://databricks.github.io/spark-deep-learning/docs/_site/quick-start.html for installation instructions.\n",
    "\n",
    "# Image Data Ingestion\n",
    "From Spark 2.3, Spark supports reading and storing images into DataFrames. We need to import ImageSchema to read images. Let's read and preprocess the images data in given code sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               image|label|\n",
      "+--------------------+-----+\n",
      "|[file:/home/dan/g...|    1|\n",
      "|[file:/home/dan/g...|    1|\n",
      "|[file:/home/dan/g...|    1|\n",
      "|[file:/home/dan/g...|    1|\n",
      "|[file:/home/dan/g...|    1|\n",
      "|[file:/home/dan/g...|    1|\n",
      "|[file:/home/dan/g...|    1|\n",
      "|[file:/home/dan/g...|    1|\n",
      "|[file:/home/dan/g...|    1|\n",
      "|[file:/home/dan/g...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "img_dir = \"personalities/\"\n",
    "\n",
    "#Read images and Create training & test DataFrames for transfer learning\n",
    "jobs_df = ImageSchema.readImages(img_dir + \"/jobs\")\n",
    "zuckerberg_df = ImageSchema.readImages(img_dir + \"/zuckerberg\")\n",
    "\n",
    "#define 1 as jobs class and 0 as zuckerberg class\n",
    "jobs_df = jobs_df.withColumn(\"label\", lit(1))\n",
    "zuckerberg_df = zuckerberg_df.withColumn(\"label\", lit(0))\n",
    "\n",
    "data=jobs_df.unionAll(zuckerberg_df)\n",
    "\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DeepImageFeaturizer_de011117e8ba, LogisticRegression_e9088ce523fa]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer\n",
    "\n",
    "# image features\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\", \n",
    "                                 outputCol=\"features\", \n",
    "                                 modelName=\"InceptionV3\")\n",
    "# model definition\n",
    "lr = LogisticRegression(maxIter=20, \n",
    "                        regParam=0.05, \n",
    "                        elasticNetParam=0.3, \n",
    "                        labelCol=\"label\")\n",
    "\n",
    "# create pipeline\n",
    "pipeline = Pipeline(stages=[featurizer, lr])\n",
    "\n",
    "pipeline.getStages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traing and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = pipeline.fit(train_df)    # train_df is a dataset of images and labels\n",
    "\n",
    "# create predictions of test_df\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.select(\"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy = 0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "predictionAndLabels = predictions.select(\"prediction\", \"label\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "print(\"Training set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Run spark using pyspark --master local[2] --packages databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data Ingestion\n",
    "Import Image schema class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "Download and preprocess [flowers data] (http://download.tensorflow.org/example_images/flower_photos.tgz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Data\n",
    "Split data into training and testing data set with 70,30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Pipeline definition\n",
    "Import required classes and define deep learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traing and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://medium.com/linagora-engineering/making-image-classification-simple-with-spark-deep-learning-f654a8b876b8\n",
    "- https://github.com/databricks/spark-deep-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-dl2]",
   "language": "python",
   "name": "conda-env-pyspark-dl2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
