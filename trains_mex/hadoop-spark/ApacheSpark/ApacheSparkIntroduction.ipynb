{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Agenda\n",
    "* What is Apache Spark?\n",
    "* Fundamentals\n",
    "* Why Spark?\n",
    "* Spark Components\n",
    "* Spark Concepts\n",
    "* Spark Lifecycle\n",
    "* Spark Stack\n",
    "* Spark Input Data\n",
    "* Spark Apps & Distributors\n",
    "* Spark Universe\n",
    "* PySpark\n",
    "* Spark Competitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# What is Apache Spark?\n",
    "\n",
    "\"Apache Spark is a fast and general engine for large-scale data processing\" https://spark.apache.org/\n",
    "\n",
    "* Open source\n",
    "* Flexible in-memory framework for batch & (near) real-time processing on clusters\n",
    "* Parallel operations and fault-tolerant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\"Scheduling, Monitoring & Distributing Engine for Big Data\", Databricks https://www.youtube.com/watch?v=7ooZ4S7Ay6Y\n",
    "\n",
    "\"Apache Spark is an open-source cluster-computing framework. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.\" (https://en.wikipedia.org/wiki/Apache_Spark)\n",
    "\n",
    "\"Spark is a fast, distributed, in-memory, large-scale processing engine\"\n",
    "https://www.youtube.com/watch?v=szB28e1A0tM\n",
    "\n",
    "* open source eco system\n",
    "* flexible in-memory framework\n",
    "* batch & real-time analytic data processing\n",
    "* high performance analytical queries\n",
    "* easy APIs: Python, Java, Scala, R\n",
    "* written in Scala\n",
    "* easy to use and code\n",
    "* based on Hadoop Storage System (HDFS) (scalable, inexpensive, reliable, any kind of data) and MapReduce (parallel processing & analytical framework) \n",
    "https://www.youtube.com/watch?v=SxAxAhn-BDU\n",
    "\n",
    "\n",
    "* fast & general-puropse cluster computing system\n",
    "* batch & real-time processing\n",
    "* large-scale data processing\n",
    "* high level APIs: Scala, Java, Python & R (Data Science)\n",
    "https://www.youtube.com/watch?v=TgiBvKcGL24\n",
    "\n",
    "\n",
    "* \"alternative to Hadoop MapRecuce\"\n",
    "* \"It’s not intended to replace Hadoop but to provide a comprehensive and unified solution to manage different big data use cases and requirements.\"\n",
    "* \"Spark takes MapReduce to the next level with less expensive shuffles in the data processing. With capabilities like in-memory data storage and near real-time processing, the performance can be several times faster than other big data technologies.\"\n",
    "https://www.infoq.com/articles/apache-spark-introduction\n",
    "\n",
    "\n",
    "Spark: Cluster Computing with Working Sets, 2010, Zaharia et. al\n",
    "* MapReduce: large-scale data-intensive applications on commodity clusters, built around acyclic data flow model!\n",
    "* Spark focus: reuse a working set of data across multiple parallel operations (such as iterative machine learning algorithms and interactive data analysis tools)\n",
    "* Spark framework: supports cyclic data flow model while retaining the scalability and fault tolerance of MapReduce\n",
    "* RDD: \"read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost\"\n",
    "* Spark Performance: \n",
    "    * can outperform Hadoop by 10x in iterative machine learning jobs\n",
    "    * interactively query a 39 GB dataset with sub-second response time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Specs\n",
    "* Developed: 2009 UC Berkeley's AMPLab by Matei Zaharia\n",
    "* Open source: 2010 under BSD license\n",
    "* Donated to Apache: 2013\n",
    "* Initial release: 2014\n",
    "* Current stable release: v2.2.0 (July 11, 2017)\n",
    "* Written in: Scala (77%), Java (10%), Python (8%), R (4%), Other (1%)\n",
    "* License: Apache License 2.0\n",
    "* Website: https://spark.apache.org/\n",
    "* Repository: https://github.com/apache/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Apache Spark Specs\n",
    "* Repository: https://github.com/apache/spark\n",
    "* Written in: Scala (76% in 2013, 9% Python, 7% Java, 8% other)\n",
    "\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Apache_Spark)\n",
    "(https://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm)\n",
    "\n",
    "## Spark Features\n",
    "* \"takes MapReduce to the next level\": \"less expensive shuffles in the data processing\"\n",
    "* in-memory data storage capabilities & near real-time processing: faster performance than other big data technologies\n",
    "* \"lazy evaluation of big data queries\": \"helps with optimization of the steps in data processing workflows\"\n",
    "* \"provides higher level API to improve developer productivity and a consistent architect model for big data solutions\"\n",
    "* \"holds intermediate results in memory rather than writing them to disk\" -> \"useful especially when you need to work on the same dataset multiple times\"\n",
    "* \"designed to be an execution engine that works both in-memory and on-disk\"\n",
    "* \"supports more than just Map and Reduce functions\"\n",
    "* \"optimizes arbitrary operator graphs\"\n",
    "* \"offers interactive shell for Scala and Python\" (no Java yet)\n",
    "https://www.infoq.com/articles/apache-spark-introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/hadoopLogo.png\" style=\"width: 200px;\"/>\n",
    "## Apache Hadoop\n",
    "\"Open-source software for reliable, scalable, distributed computing\" http://hadoop.apache.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/hadoopLogo.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    " | Hadoop 1.x | Hadoop 2.x | []() |\n",
    "- | - | - | -\n",
    " | | <img align=\"right\" src=\"img/mapReduceLogo.png\" style=\"width: 200px;\"/> | Data Processing\n",
    "Resource Management & Data Processing | <img align=\"left\" src=\"img/mapReduceLogo.png\" style=\"width: 200px;\"/> | <img align=\"right\" src=\"img/yarnLogo.png\" style=\"width: 200px;\"/> | Resource Management\n",
    "File System | <img align=\"left\" src=\"img/hdfsLogo.png\" style=\"width: 200px;\"/> |<img align=\"right\" src=\"img/hdfsLogo.png\" style=\"width: 200px;\"/> | File System\n",
    "\n",
    "Hadoop 3.0.0-beta1 published on October 3, 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Hadoop 3:\n",
    "* increased minimum required Java version from 7 to 8\n",
    "* support for erasure encoding in HDFS: space savings\n",
    "* YARN Timeline Service v.2: \"improving scalability and reliability of Timeline Service, and enhancing usability by introductin flows and aggregation\"\n",
    "* Shell script rewrite: fix bugs and include more features\n",
    "* Shaded client jars\n",
    "* Support for opportunistic containers and distributed scheduling\n",
    "* MapReduce task-level native optimization: up to 30% performance improvement for shuffle-intensive jobs\n",
    "* Support for more than 2 NameNodes\n",
    "* Changing default ports for multiple services\n",
    "* Support for Microsoft Azre Data Lake and Aliyun Object Storage System filesystem connectors\n",
    "* Intra-datanode balancer\n",
    "* Reworked daemon and task heap management: auto-tuning based on heap size of host\n",
    "* S3Guard: Consistency and Metadata Caching for the S3A filesystem client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/hdfsLogo.png\" style=\"width: 200px;\"/>\n",
    "### Apache HDFS\n",
    "* Distributed file system\n",
    "* Any kind of data\n",
    "* **Inexpensive:** runs on commodity hardware\n",
    "* **Reliable:** highly fault-tolerant\n",
    "* **Scalable:** high throughput for large datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/hdfsLogo.png\" style=\"width: 200px;\"/>\n",
    "<img src=\"img/hdfsArchitecture.gif\" style=\"width: 700px;\"/>\n",
    "https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/yarnLogo.png\" style=\"width: 200px;\"/>\n",
    "### YARN (Yet Another Resource Nagivator)\n",
    " \"Framework for job scheduling and cluster resource management\" https://hadoop.apache.org/\n",
    "* Integrated into Hadoop 2.0\n",
    "* Allows multiple applications to run on the same platform\n",
    "* Components:\n",
    "    * ResourceManager\n",
    "    * NodeManager\n",
    "    * ApplicationMaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/yarnLogo.png\" style=\"width: 200px;\"/>\n",
    "<img src=\"img/yarn-clientMode.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/mapReduceLogo.png\" style=\"width: 200px;\"/>\n",
    "### MapReduce\n",
    "\"YARN-based system for parallel processing of large datasets\" https://hadoop.apache.org/\n",
    "\n",
    "* Written in **Java**\n",
    "* MapReduce job:\n",
    "    1. Read input data and split input dataset into independent chunks and distribute them\n",
    "    2. Map task\n",
    "    3. Shuffle: sort outputs of the maps (=input of reduce tasks)\n",
    "    4. Reduce task\n",
    "    5. Write results to disk\n",
    "* User: specify input/output location and map & reduce functions\n",
    "* **Batch processing**\n",
    "* Build around an **acyclic data flow model**: one-pass computations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MapReduce vs Spark in the Apache Universe\n",
    "\n",
    "| MapReduce for Batch Processing | Spark for In-memory Processing\n",
    "- | - | - \n",
    "Data Processing | <img src=\"img/mapReduceLogo.png\" style=\"width: 200px;\"/> | <img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "Resource Management | <img src=\"img/yarnLogo.png\" style=\"width: 200px;\"/> | <img src=\"img/yarnLogo.png\" style=\"width: 200px;\"/>\n",
    "File System | <img src=\"img/hdfsLogo.png\" style=\"width: 200px;\"/> |<img src=\"img/hdfsLogo.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Fundamentals / History:\n",
    "\n",
    "Why do we need Spark?\n",
    "What did we have before Spark?\n",
    "* HDFS, YARN (since Hadoop 2.0) and MapReduce (and others) -> BATCH processing\n",
    "\n",
    "What did we lack?\n",
    "Applications that reuse a working set of data across multiple parallel operations (IN-MEMORY processing):\n",
    "* Iterative jobs: \"machine learning alrogithms that apply a function repeatedly to the same dataset to optimize a parameter\". each iteration -> one MapReduce job -> each job reloads the data from disk\n",
    "* Iterative analytics: \"ad-hoc exploratory queries on large datasets, through SQL interfaces such as Pig and Hive\", \"each query incures significant latency (...) because it runsas a separate MapReduce job and reads data from disk\" -> ideally: \"load a dataset of interest into memory across a number of machines and query it repeatedly\"\n",
    "\n",
    "## Apache Hadoop \n",
    "* toolkit: \"open-source software for reliable, scalable, distributed computing.\" http://hadoop.apache.org/\n",
    "* \"Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.\" http://hadoop.apache.org/\n",
    "* all data in one place (HDFS)\n",
    "* batch processing only with MapReduce\n",
    "* Basic Modules:\n",
    "    * 1.0: MapReduce on top of HDFS\n",
    "    * 2.0: YARN in between MapReduce and HDFS\n",
    "\n",
    "### HDFS\n",
    "\"a distributed file system that provides high-throughput access to application data.\" https://hadoop.apache.org/\n",
    "as distributed file storage system\n",
    "* scalable\n",
    "* any kind of data\n",
    "* inexpensive\n",
    "* reliable\n",
    "\n",
    "### YARN\n",
    "= Yet Another Resource Nagivator\n",
    "* \"framework for job scheduling and cluster resource management.\" https://hadoop.apache.org/\n",
    "* used for Hadoop 2.x and Spark for managing resources in the cluster\n",
    "* integrated into Hadoop 2.0, allows multiple applications to run on the same platform\n",
    "* components (replaced JobTracker and TaskTracker in Hadoop 1.x):\n",
    "    * ResourceManager: scheduler that allocates available resources in the cluster (among competing applications)\n",
    "    * NodeManager: runs on each node to manage resources available on a single node and take direction from the ResourceManager\n",
    "    * ApplicationMaster: \"instance of a framework-specific library\", \"actual data processing occures within the Containers executed by the ApplicationMaster\", \"A Container grants rights to an application to use a specific amount of resources\"\n",
    "    \n",
    "https://www.oreilly.com/ideas/an-introduction-to-hadoop-2-0-understanding-the-new-data-operating-system\n",
    "\n",
    "<img src=\"img/yarn-clientMode.png\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "* Running Spark in YARN\n",
    "    * client mode: interactive working, driver on client (terminates if client disconnects)\n",
    "    * cluster mode: driver on AppMaster\n",
    "\n",
    "\n",
    "### MapReduce\n",
    "\"YARN-based system for parallel processing of large data sets.\" https://hadoop.apache.org/\n",
    "* (parallel) processing and analysis framework\n",
    "* build around an acyclic data flow model\n",
    "* direct \"competitor\" to Spark\n",
    "* more parallelism: more process IDs (Spark: more threads running inside executer)\n",
    "* drawbacks:\n",
    "    * many little steps -> code (lot of) Java code\n",
    "    * APIs\n",
    "    \n",
    "\"MapReduce is a great solution for one-pass computations, but not very efficient for use cases that require multi-pass computations and algorithms. Each step in the data processing workflow has one Map phase and one Reduce phase and you'll need to convert any use case into MapReduce pattern to leverage this solution.\n",
    "\n",
    "The Job output data between each step has to be stored in the distributed file system before the next step can begin. Hence, this approach tends to be slow due to replication & disk storage. Also, Hadoop solutions typically include clusters that are hard to set up and manage. It also requires the integration of several tools for different big data use cases (like Mahout for Machine Learning and Storm for streaming data processing).\n",
    "\n",
    "If you wanted to do something complicated, you would have to string together a series of MapReduce jobs and execute them in sequence. Each of those jobs was high-latency, and none could start until the previous job had finished completely.\"\n",
    "https://www.infoq.com/articles/apache-spark-introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "# Why Spark?\n",
    "\n",
    "* Speed\n",
    "* Ease of use\n",
    "* Generality\n",
    "* Runs everywhere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Speed\n",
    "\"Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk\" https://spark.apache.org/\n",
    "* Next Gen Shuffle\n",
    "    * 100 TB Daytona Sort Competition 2014\n",
    "    * Sorting on disk (HDFS)\n",
    "    * 3x faster using 10x fewer machines than Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Next Gen Shuffle (operations that tigger shuffle: groupByKey, sortByKey, reduceByKey)\n",
    "    * 100 TB Daytona Sort Competition\n",
    "    * sorting on disk (HDFS), no in-memory cache used for map-phase (in-memory was only used for reduce-phase)\n",
    "    * 3x faster using 10x fewer machines than Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Iterative Process in MapReduce and Spark\n",
    "<img src=\"img/mrAndSparkProcess.png\" style=\"width: 750px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ease of Use\n",
    "Word count in Spark's Python API:\n",
    "    \n",
    "```python\n",
    "text_file = spark.textFile(\"hdfs://...\")\n",
    " \n",
    "text_file.flatMap(lambda line: line.split())\n",
    "    .map(lambda word: (word, 1))\n",
    "    .reduceByKey(lambda a, b: a+b)\n",
    "```\n",
    "https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generality\n",
    "\"Combine SQL, streaming, and complex analytics\" https://spark.apache.org/\n",
    "<img src=\"img/fromMrToSpark.png\" style=\"width: 750px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Runs Everywhere\n",
    "* Local\n",
    "* Standalone\n",
    "* Hadoop Yarn\n",
    "* Apache Mesos\n",
    "\n",
    "https://spark.apache.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\"Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.\" (https://spark.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Components\n",
    "## Driver Program (SparkContext object)\n",
    "* Process running the user code (creates SparkContext)\n",
    "    * Converting a program to tasks (Direct Acyclic Graph (DAG))\n",
    "        * Logical sequence of operations\n",
    "        * *\"The magic behind Spark\"*\n",
    "    * Scheduling tasks on executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Spark Components\n",
    "## Executors & Worker Nodes\n",
    "* One worker node per machine but multiple executors per worker node possible (parallelism)\n",
    "* Executor tasks:\n",
    "    * Run the individual tasks and return the results to the driver\n",
    "    * Provide in-memory storage\n",
    "* Local mode: Spark  driver and executor run in the same Java process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Spark Components\n",
    "## Cluster Manager \n",
    "* Resource allocation\n",
    "* Communicates with executors and driver\n",
    "* Partitioning:\n",
    "    * Static\n",
    "       * **Local:** local client for prototyping & testing\n",
    "       * **Standalone:** Spark built-in cluster manager\n",
    "    * Dynamic\n",
    "       * **Hadoop Yarn:** Hadoop 2 cluster manager\n",
    "       * **Apache Mesos:** General cluster manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Spark Components\n",
    "\n",
    "<img src=\"img/clusterMode.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Spark Components Notes\n",
    "Driver program, Cluster manager, Worker Nodes (https://www.youtube.com/watch?v=TgiBvKcGL24)\n",
    "\n",
    "distributed computation engine\n",
    "\n",
    "sparkComponents.png (without cluster manager)\n",
    "\n",
    "* written by developers: implements the high-level control flow of the application and launches various operations in parallel\n",
    "* runs on one node next to executor(s)\n",
    "\n",
    "dynamic partitioning: grow&reduce executors live\n",
    "\n",
    "https://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm https://www.youtube.com/watch?v=7ooZ4S7Ay6Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "## Hadoop -> Specialized Systems -> Spark\n",
    "\n",
    "2004-2013: Hadoop MapReduce for general batch processing\n",
    "\n",
    "2007-2015: Specialized systems such as\n",
    "* Storm\n",
    "* Mahout\n",
    "* S4\n",
    "* GraphLab\n",
    "* Tez\n",
    "* Drill\n",
    "* Giraph\n",
    "* Pregel\n",
    "* Dremel\n",
    "* Impala\n",
    "* ...\n",
    "\n",
    "2014-?: Spark as general unified engine\n",
    "* \n",
    "\n",
    "MapReduce and Spark Process ([] for repeated cycle):\n",
    "\n",
    "Hadoop -> MapReduce -> [ Hadoop -> MapReduce -> ] -> Hadoop\n",
    "\n",
    "Hadoop -> In-memory -> Spark -> [ In-memory -> Spark -> ] -> Cassandra\n",
    "(10-100x faster because of in-memory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Concepts\n",
    "* RDDs\n",
    "* Parallel Operations\n",
    "* Shared Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RDDs (Resilient Distributed Datasets)\n",
    "* Main abstraction in Spark\n",
    "* **Read-only** collection of objects\n",
    "* **Partioned** across a set of machines  (more partitions = more parallelism)\n",
    "* **Fault-tolerant:** can be rebuilt if a partition is lost\n",
    "* **Lazy:** only computed when used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RDDs (Resilient Distributed Datasets)\n",
    "* Two types of operations:\n",
    "    * **Transformations:** return a new RDD (e.g. filter)\n",
    "    * **Actions:** start a computation and return a result (to driver program or write it to storage)\n",
    "* Can be created:\n",
    "    * **Parallelizing** a collection: ```sc.parallelize([\"a\", \"b\", \"c\"])```\n",
    "    * **Read data** from disk: e.g. ```sc.textFile(\"/path/f.md\")```\n",
    "    * **Transforming** an existing RDD\n",
    "    * Change the **persistence** of an existing RDD:\n",
    "        * cache()\n",
    "        * save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RDDs (Resilient Distributed Datasets)\n",
    "<img src=\"img/rdd.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parallel Operations\n",
    "* Set of parallel tasks on different nodes\n",
    "* Copy of each variable used in the function is shipped to each tasks\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Shared Variables\n",
    "\"across tasks, or between tasks and the driver program\" for parallel operations\n",
    "\n",
    "* **Broadcast variables:**\n",
    "    * Cached in memory on all nodes\n",
    "    * Only copied once to every worker (e.g. for large read-only lookup tables)\n",
    "* **Accumulators:**\n",
    "    * Workers can only \"add\"\n",
    "    * Only the driver can read (e.g. for counters)\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Parallel Operations Notes\n",
    "\n",
    "\"By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task\"\n",
    "\n",
    "* reduce: \"combines dataset elements using an associative function to produce a result at the driver program\"\n",
    "* collect: \"sends all elements of the dataset to the driver program\"\n",
    "* foreach: \"passes each element through a user provided function\"\n",
    "\n",
    "* suffle?: \"re-distributing data so that it’s grouped differently across partitions\"\n",
    "\n",
    "Shared Variables Notes\n",
    "\n",
    "Only copied once to every worker and not distributed for every closure/function call (e.g. for large read-only lookup tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Lifecycle\n",
    "\n",
    "1. Create input data source in **RDD** (external or parallelized)\n",
    "2. (Lazy) **transformations**: define new RDDs\n",
    "3. [Optional: **cache()** any intermediate RDD for reuse]\n",
    "4. (Parallel & optimized) **actions**\n",
    "5. Processed data **RDD** / UI dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Stack Overview\n",
    "\n",
    "<img src=\"img/sparkStack.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Core APIs\n",
    "* Scala\n",
    "* Java\n",
    "* Python\n",
    "* R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Stack Overview\n",
    "\n",
    "<img src=\"img/sparkStack.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Spark Higher-Level APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark SQL, DataFrames & Datasets\n",
    "#### Spark SQL  http://spark.apache.org/sql/\n",
    "* Module for working with structured data\n",
    "    * **Integrated:** SQL queries within Spark programs\n",
    "    * **Uniform data access:** connect to any data source\n",
    "    * **Hive integration:** SQL or HiveQL queries\n",
    "    * **Standard connectivity:** through JDBC or ODBC\n",
    "    \n",
    "<img src=\"img/sparkSQL.png\" style=\"width: 650px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Spark SQL\n",
    "* csv, json, sql-like repository https://www.youtube.com/watch?v=TgiBvKcGL24\n",
    "* \"allow running the SQL like queries on data using traditional BI and visualization tools\"\n",
    "* \"allows the users to ETL their data from different formats (...), transform it, and expose it for ad-hoc querying\" https://www.infoq.com/articles/apache-spark-introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark SQL, DataFrames & Datasets\n",
    "#### RDD Recap\n",
    "* Low level type-safty (data field types)\n",
    "* Java / Kyrp serialization\n",
    "    * Distribute data in network\n",
    "    * Write data to disk\n",
    "* Overhead: class structure & values for every record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark SQL, DataFrames & Datasets\n",
    "#### DataFrame\n",
    "\n",
    "* **Higher level abstraction** (than RDD) introduced in Spark 1.3\n",
    "* \"Conceptually equivalent to a table in a relational database or a DataFrame in R/Python\"\n",
    "* Schema managed by Spark (**column types**)\n",
    "* Faster than RDD but **lost type safety**\n",
    "    * only data (without class structure) send or written\n",
    "    * optimized relational query plan by Spark's Catelyst optimizer\n",
    "* DataFrame API in Scala, Java, Python and R\n",
    "    * Similar to Pandas and R\n",
    "    * DataFrame is a Dataset of untyped Rows:\n",
    "        * Scala: ```DataFrame = Dataset[Row]```\n",
    "        * Java: ```DataFrame = Dataset<Row>```\n",
    "\n",
    "http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "DataFrame notes\n",
    "* Faster than RDD (make use of the schema to store data) but lost type safety (dynamically-typed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark SQL, DataFrames & Datasets\n",
    "#### Dataset\n",
    "* **\"Distributed collection of data\"**\n",
    "* Build on top of RDD (since Spark 1.6)\n",
    "* **Strongly-typed** to keep track of their schema: ```Dataset[T]```\n",
    "* Combines benefits of RDDs (strong typing & lambda functions) and DataFrames (Spark SQL's optimized execution engine)\n",
    "* **Tungsten in-memory encoding**: use less memory & fast (de-)serialization\n",
    "* Manipulated using functional transformations \n",
    "* Dataset API available in Scala and Java\n",
    "* Python and R already have many of these benefits by nature (row.columName) but only provide untyped objects\n",
    "\n",
    "http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark SQL, DataFrames & Datasets\n",
    "<img src=\"img/datasetCode.png\" style=\"width: 750px;\"/>\n",
    "https://github.com/bmc/rdds-dataframes-datasets-presentation-2016\n",
    "* Spark 2.x unifies concepts of Datasets and DataFrames\n",
    "    * Untyped (```Dataset[Row]```) and Typed (```Dataset[T]```) Dataset API\n",
    "* Spark 3.x is expected to remove the RDD API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark SQL, DataFrames & Datasets\n",
    "\n",
    "**DataFrames and Datasets:** detect syntax and analysis errors at compile time\n",
    "\n",
    "| errors \\ API | SQL | DataFrames | Datasets |\n",
    "|--------------|-----|------------|----------|\n",
    "| Syntax       | R   | C          | C        |\n",
    "| Analysis     | R   | R          | C        |\n",
    "\n",
    "R = runtime, C = compile time\n",
    "\n",
    "http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar.\n",
    "\n",
    "A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.\n",
    "\n",
    "http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark Streaming\n",
    "\"Build scalable fault-tolerant streaming applications\" http://spark.apache.org/streaming/\n",
    "* **Micro-batching:** (near) real-time processing\n",
    "* **Ease of use:** high-level operators\n",
    "* **Fault-tolerance:** recovers lost work & operator state\n",
    "* **Spark integration:** combine streaming with batch & interactive queries\n",
    "* Unified API for batch (Hadoop MapReduce) and realtime (Apache Storm) processing\n",
    "\n",
    "<img src=\"img/sparkStreaming.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark Streaming\n",
    "#### DStream (discretized stream)\n",
    "* High level abstraction\n",
    "* Continuous input data stream\n",
    "* Series of RDDs \n",
    "* Batch interval: x seconds \n",
    "    * new RDD is created every x seconds\n",
    "    * min. 0.5s (90% of the use cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Spark Streaming Notes\n",
    "* Complex algorithms can be expressed using:\n",
    "    * Spark transformations: map(), reduce(), join(), ...\n",
    "    * MLlib + GraphX\n",
    "    * SQL\n",
    "https://www.youtube.com/watch?v=TgiBvKcGL24\n",
    "https://www.infoq.com/articles/apache-spark-introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MLlib\n",
    "\"Scalable machine learning library\" http://spark.apache.org/mllib/\n",
    "* **Easy to use:** Java, Scala, Python, R\n",
    "* **Performance:** up to 100x faster than MapReduce\n",
    "* **Easy to deploy:** runs on existing Hadoop clusters & data\n",
    "* Set of functions to call on DataFrames\n",
    "* Only contains parallel algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "MLlib Notes\n",
    "* \"Scalable machine learning library consisting of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, as well as underlying optimization primitives.\" https://www.infoq.com/articles/apache-spark-introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/graphxLogo.png\" style=\"width: 200px;\"/>\n",
    "###  GraphX\n",
    "\"API for graphs & graph-parallel computation\"\n",
    "* Graph abstraction on top of RDDs\n",
    "* **Flexibility:** works with graphs & collections\n",
    "* **Speed:** comparable performance with fastest specialized graph processing systems\n",
    "* **Algorithms:** growing library of graph algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "GraphX Notes\n",
    "* \"New (alpha) Spark API for graphs and graph-parallel computation. At a high level, GraphX extends the Spark RDD by introducing the Resilient Distributed Property Graph: a directed multi-graph with properties attached to each vertex and edge. To support graph computation, GraphX exposes a set of fundamental operators (e.g., subgraph, joinVertices, and aggregateMessages) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.\" https://www.infoq.com/articles/apache-spark-introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### External Projects (extract)\n",
    "\n",
    "* **SparkR:** R frontend for Spark\n",
    "* **EclairJS:** use Jupyter notebooks for Spark\n",
    "* **BlinkDB:** approximate query engine\n",
    "* **Tachyon:** memory speed virtual distributed storage system\n",
    "\n",
    "http://spark.apache.org/third-party-projects.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "External Projects Notes\n",
    "see http://spark.apache.org/third-party-projects.html\n",
    "* **BlinkDB**\n",
    "\"approx. query engine (...) used for running interactive SQL queries on large volumes of data. It allows users to trade-off query accuracy for response time\" https://www.infoq.com/articles/apache-spark-introduction\n",
    "approx results with error bar\n",
    "* **Tachyon**\n",
    "\"memory-centric distributed file system enabling reliable file sharing at memory-speed across cluster frameworks, such as Spark and MapReduce. It caches working set files in memory, thereby avoiding going to disk to load datasets that are frequently read. This enables different jobs/queries and frameworks to access cached files at memory speed.\" https://www.infoq.com/articles/apache-spark-introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Input Data\n",
    "\"Anything that has a Hadoop Input Format\"\n",
    "* **File Systems**\n",
    "    * Local Ext3/4\n",
    "    * HDFS\n",
    "    * Amazon S3\n",
    "    * OpenStack Swift\n",
    "* **SQL & NoSQL Databases**\n",
    "    * RDBMS\n",
    "    * Cassandra\n",
    "    * MongoDB\n",
    "    * HBase\n",
    "    * Neo4j\n",
    "* **Buffers for Spark Streaming**\n",
    "    * Flume\n",
    "    * Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Apps & Distributors\n",
    "## Apps\n",
    "* Tableau\n",
    "* elasticsearch\n",
    "* ...\n",
    "\n",
    "## Distributors\n",
    "* Datastax\n",
    "* Cloudera\n",
    "* Hartonworks\n",
    "* MapR\n",
    "* IBM\n",
    "* SAP\n",
    "* Oracle\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Universe\n",
    "<img src=\"img/sparkUniverse.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Spark Core: uses in-memory and disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Underlying conceps\n",
    "## Programming Model\n",
    "\"Two main abstractions for parallel programming in Spark: RDDs and parallel operations on these datasets (...). In addition, Spark supports two restricted types of shared variables that can be used in functions running on the cluster (...).\" (Spark: Cluster Computing with Working Sets, 2010, Zaharia et. al)\n",
    "\n",
    "\n",
    "### RDD\n",
    "=Resilent Distributed Dataset\n",
    "* read-only collection ob objects\n",
    "* partitioned across a set of machines\n",
    "* can be rebuilt if a partition is lost\n",
    "* more partitions = more parallelism\n",
    "* \"elements of an RDD need not exist in phyiscal storage\"\n",
    "* \"a handle in an RDD contains enough information to compute the RDD starting from data in reliable storage\"\n",
    "* \"RDDs can always be reconstructed if nodes fail\"\n",
    "* lazy and ephemeral (\"lasting for a very short time\") by default\n",
    "* can be created/constructed:\n",
    "    * parallelizing a collection: SparkContext.parallelize(existing in-memory collection) for prototyping and testing puriposes (requires entire dataset in-memory on one machine)\n",
    "    * read data from file/external source: sc.textFile(\"/path/fileToRead.md\") and other methods for reading data from HDFS, C*, S3, HBase,...\n",
    "    * transforming an existing RDD (using the flatMap() operation with a user-provided function)\n",
    "    * change the persistence of an existing RDD\n",
    "        * cache(): leave the dataset lazy but keep it in memory for later reuse (cache action is just a hint. if there is not enough memory in the cluster, the dataset cannot be kept in memory and will be recomputed on demand)\n",
    "        * save(): write dataset to disk (such as HDFS)\n",
    "* Types\n",
    "    * HadoopRDD\n",
    "    * FilteredRDD\n",
    "    * MappedRDD\n",
    "    * PairRDD\n",
    "    * ShuffledRDD\n",
    "    * UnionRDD\n",
    "    * PythonRDD\n",
    "    * DoubleRDD\n",
    "    * JdbcRDD\n",
    "    * JsonRDD\n",
    "    * SchemaRDD\n",
    "    * VertexRDD\n",
    "    * EdgeRDD\n",
    "    * CassandraRDD (DataStax)\n",
    "    * GeoRDD (ESRI)\n",
    "    * EsSpark (Elasticsearch)\n",
    "    * ...\n",
    "* RDD.scala: abstract class, defines more types on git\n",
    "* RDD interface:\n",
    "    * set of paritions (\"splits\")\n",
    "    * list of dependencies (on parent RDD)\n",
    "    * recreate function (to compute a partition given parents)\n",
    "    * [preferred locations]\n",
    "    * [partitioning info for k/v RDDs (Partitioner)]\n",
    "\n",
    "### Parallel Operations\n",
    "* reduce: \"combines dataset elements using an associative function to produce a result at the driver program\"\n",
    "* collect: \"sends all elements of the dataset to the driver program\"\n",
    "* foreach: \"passes each element through a user provided function\"\n",
    "\n",
    "* suffle?\n",
    "\n",
    "### Shared Variables\n",
    "* broadcast variables: as used in multiple parallel operations. Only copied once to every worker and not distributed for every closure/function call (e.g. for large read-only lookup tables)\n",
    "* accumulator: variables that workers can only \"add\" and only the driver can read (e.g. for counters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Apache Spark Core / Architecture (lowest level in stack)\n",
    "## Data Storage\n",
    "HDFS and other \"Hadoop compatible data source including HDFS, HBase, Cassandra, etc.\"\n",
    "### RDD (Resilent Distributed Datasets)\n",
    "\n",
    "\n",
    "\n",
    "## Management Framework\n",
    "Distributed Computing\n",
    "(https://www.infoq.com/articles/apache-spark-introduction)\n",
    "\n",
    "\n",
    "Driver Program (Interactive Shell for Scala & Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Apache Spark Deployment\n",
    "\"runs on top of existing HDFS intrastructure to provide enhanced and additional functionality\" https://www.infoq.com/articles/apache-spark-introduction\n",
    "\n",
    "\n",
    "## Distributed Storage System\n",
    "* HDFS\n",
    "* MapR-FS\n",
    "* Cassandra\n",
    "* OpenStack Swift\n",
    "* Amazon S3\n",
    "* Kudu\n",
    "* Custom implementation\n",
    "* Pseudo-distributed local mode (for development & testing)\n",
    "https://en.wikipedia.org/wiki/Apache_Spark\n",
    "\n",
    "\n",
    "## Configuration\n",
    "\"more an art than a science\"\n",
    "* use max 75% of machine's memory for Spark\n",
    "* executor heapsize: 8-40GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Spark Concepts\n",
    "\n",
    "## RDD\n",
    "Resilent Distributed Dataset\n",
    "## DAG\n",
    "Direct Acyclic Graph: Sequence of computations / execution flow (\"magic behind Spark)\n",
    "## SparkContext\n",
    "Driver: manages orchestration in clusters\n",
    "## Transformations\n",
    "Load data into (immutable) RDD, filter/modify data triggers new RDD creation\n",
    "## Actions\n",
    "\"lazy loading\": computation/exeution only done when required, request triggers processing: \"resilence\"\n",
    "https://www.youtube.com/watch?v=TgiBvKcGL24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark\n",
    "\n",
    "## PySpark Stack\n",
    "* PySpark sits on top of the Java API\n",
    "* Call Java methods from Python code (as if they where Python methods)\n",
    "<img src=\"img/PySpark-stack.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PySpark Architecture\n",
    "* Pipe on worker machine: custom pipe for high throughput\n",
    "* Do not read data in Python: would be high I/O so executor JVM on Worker machine reads directly\n",
    "* Py4j is slow for high throughput: use local disk to ship datasets at driver machine\n",
    "<img src=\"img/PySpark-Architecture.png\" style=\"width: 750px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## PythonRDD\n",
    "<img src=\"img/pythonRDD.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Python Implementation\n",
    "* CPython (default)\n",
    "* pypy (use if you don't need C libraries, if you just use plain Python code)\n",
    "    * JIT: 20-3000% faster\n",
    "    * Less memory\n",
    "    * CFFI (C Foreign Function Interface) support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/sparkLogo.png\" style=\"width: 200px;\"/>\n",
    "# Spark Competitors (extract)\n",
    "* Apache MapReduce\n",
    "* Apache Flink: Stream and batch data processing\n",
    "* **SQL**\n",
    "    * Apache Hive\n",
    "    * Apache Pig\n",
    "* **Streaming**\n",
    "    * Apache Storm\n",
    "    * Apache Apex\n",
    "    * Apache Samza\n",
    "* **Machine Learning**\n",
    "    * Apache Mahout\n",
    "* **Graph**\n",
    "    * Apache Giraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
