{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming\n",
    "Spark Structured Streaming is a new feature introduced in Spark 2.0 which provides the same API for streaming applications as available for DataFrames in batch applications. Spark Structured Streaming provides a unified API for streaming applications which are more efficient and easy to use compared to the old Spark Streaming API that is based on RDDs. Spark Streaming allows the user to run continuous queries over the streaming data. In recents years, there was a gap to run simple SQL-like queries on structured but continuous data. Spark introduced Structured Streaming to help Data Analysts to run SQL-like simple queries on streams of data. It provides all the basic features of RDD-based streaming and also provides an SQL interface based on the Spark DataFrame API. In addition, Spark Structured Streaming provides features such as:\n",
    "* Streaming aggregations\n",
    "* Continuous window aggregations\n",
    "* Stateful streaming aggregations\n",
    "* Watermarks for streaming to handle late events\n",
    "\n",
    "In Structured Streaming users can perform SQL-like operations and Spark internally makes sure that they run continuously on infinite data streams. A Structured Streaming job takes data from a streaming source, applies different transformations and operations on the streaming dataset and writes the results to some external data sink. With Spark Structured Streaming the user can write to sinks in the following write modes:\n",
    "* **Append**  : Write new rows only\n",
    "* **Update**  : Write the rows that were updated\n",
    "* **Complete**: Write all rows including updated and new rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal and run the following command:\n",
    "\n",
    "*nc -lk 9999*\n",
    "\n",
    "![nc-lk](nc-lk.png)\n",
    "\n",
    "Run the cell below and insert data into the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StructuredNetworkWordCount\").getOrCreate()\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select( explode( split(lines.value, \" \") ).alias(\"word\") )\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "\n",
    " # Start running the query that prints the running counts to the console\n",
    "query = wordCounts.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "\n",
    "query.awaitTermination(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to stop the Structured Streaming Application\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Streaming Data with Static Data\n",
    "In this example, customer information and product information is static data while the transaction information is streamed. For each incoming new transaction, we would like to join the static customer and product information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------+---+\n",
      "|cust_id|           name|gender|age|\n",
      "+-------+---------------+------+---+\n",
      "|      1|    Tawsha Haig|Female| 26|\n",
      "|      2|Sayres Aiskrigg|  Male| 39|\n",
      "|      3|    Tate Metham|  Male| 54|\n",
      "|      4|   Fanya Torres|Female| 38|\n",
      "|      5| Callie Perrigo|Female| 39|\n",
      "+-------+---------------+------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+--------------------+-----+\n",
      "|p_id|        product_name|price|\n",
      "+----+--------------------+-----+\n",
      "|   1|Sping Loaded Cup ...|   16|\n",
      "|   2|Mustard - Individ...|   16|\n",
      "|   3|      Shrimp - Prawn|    9|\n",
      "|   4|  Bread Country Roll|   23|\n",
      "|   5|Flower - Dish Garden|   26|\n",
      "+----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "cusomersFile = \"./input/customers.csv\"\n",
    "productsFile = \"./input/products.csv\"\n",
    "transactionsData = \"./input/transactions\"\n",
    "\n",
    "cusomersDF = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "                  .option(\"inferSchema\", \"true\").load(cusomersFile)\n",
    "cusomersDF.show(5)\n",
    "\n",
    "productsDF = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "                  .option(\"inferSchema\", \"true\").load(productsFile)\n",
    "productsDF.show(5)\n",
    "\n",
    "schema = StructType([ StructField(\"t_id\", IntegerType(), True), \n",
    "                     StructField(\"p_id\", IntegerType(), True),\n",
    "                     StructField(\"cust_id\", IntegerType(), True) ])\n",
    "\n",
    "transactionsDF =spark.readStream.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                     .option(\"maxFilesPerTrigger\", 1).schema(schema)\\\n",
    "                     .load(transactionsData)\n",
    "\n",
    "salesPerCustomer= transactionsDF.join(cusomersDF,\"cust_id\").join(productsDF,\"p_id\")\\\n",
    "                                .groupBy(\"cust_id\").sum(\"price\").alias(\"sales\")\n",
    "\n",
    " # start running the query that prints the running counts to the console\n",
    "query = salesPerCustomer.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "\n",
    "query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the terminal to see the cust_id and the sum of the prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [structured-streaming-programming-guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "2. [introduction-to-spark-structured-streaming-part-6](http://blog.madhukaraphatak.com/introduction-to-spark-structured-streaming-part-6/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark]",
   "language": "python",
   "name": "conda-env-pyspark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
