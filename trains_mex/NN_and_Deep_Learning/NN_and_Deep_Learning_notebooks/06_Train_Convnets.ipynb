{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Train Convnets\n",
    "**Adapted from Deep Learning with Pyton by Francois Chollet**\n",
    "\n",
    "https://github.com/fchollet/deep-learning-with-python-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Convnet from on a Small Dataset \n",
    "\n",
    "* Open a Kaggle account.\n",
    "* Go to https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "* Click on `Download All` to download the dataset.\n",
    "* Unzip the files.\n",
    "* Copy the unziped folders to a directory named `kaggle_original_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training archive contains 25,000 medium-resolution color JPEG images of dogs and cats. \n",
    "\n",
    "The goal is to train an algorithm on these files and predict the labels for test1.zip (1 = dog, 0 = cat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to the directory where the original dataset was uncompressed\n",
    "original_dataset_dir = '/home/mabeigier/Downloads/kaggle_original_data/train'\n",
    "\n",
    "# the directory where we will store our smaller dataset\n",
    "\n",
    "base_dir = '/home/mabeigier/Downloads/cats_and_dogs_small'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# directories for our training, validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# directory with training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "# directory with training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# directory with validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "# directory with validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# directory with test cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "# directory with test dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "# copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# copy the next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# copy the next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# copy the next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# copy the next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images', len(os.listdir(train_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training dog images 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training dog images', len(os.listdir(train_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation cat images 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation cat images', len(os.listdir(validation_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation dog images 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation dog images', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test cat images 500\n"
     ]
    }
   ],
   "source": [
    "print('total test cat images', len(os.listdir(test_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test dog images 500\n"
     ]
    }
   ],
   "source": [
    "print('total test dog images', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network\n",
    "\n",
    "A convnet like a stack of alternated `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
    "\n",
    "We start from inputs of size 150x150 (a somewhat arbitrary choice), we end up with feature maps of size 7x7 right before the `Flatten` layer.\n",
    "\n",
    "The depth of the feature maps is progressively increasing in the network (from 32 to 128), while the size of the feature maps is decreasing (from 148x148 to 7x7). This is a pattern in almost all convnets.\n",
    "\n",
    "This is a binary classification problem so we are ending the network with a single unit (a `Dense` layer of size 1) and a `sigmoid` activation. This unit will encode the probability that the network is looking at one class or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0815 11:56:43.228487 139817182578496 deprecation_wrapper.py:119] From /home/mabeigier/anaconda3/envs/sklearn/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0815 11:56:43.259387 139817182578496 deprecation_wrapper.py:119] From /home/mabeigier/anaconda3/envs/sklearn/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0815 11:56:43.266082 139817182578496 deprecation_wrapper.py:119] From /home/mabeigier/anaconda3/envs/sklearn/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0815 11:56:43.351610 139817182578496 deprecation_wrapper.py:119] From /home/mabeigier/anaconda3/envs/sklearn/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RMSprop` optimizer and `binary_crossentropy` as loss since the network ends with a single sigmoid unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0815 11:56:50.132941 139817182578496 deprecation_wrapper.py:119] From /home/mabeigier/anaconda3/envs/sklearn/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0815 11:56:50.143176 139817182578496 deprecation_wrapper.py:119] From /home/mabeigier/anaconda3/envs/sklearn/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0815 11:56:50.149583 139817182578496 deprecation.py:323] From /home/mabeigier/anaconda3/envs/sklearn/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Data should be formatted into appropriately pre-processed floating point tensors before being fed into the \n",
    "network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are:\n",
    "\n",
    "* Read the picture files.\n",
    "* Decode the JPEG content to RBG grids of pixels.\n",
    "* Convert these into floating point tensors.\n",
    "* Rescale the pixel values (between 0 and 255) to the [0, 1] interval (neural networks prefer to deal with small input values).\n",
    "\n",
    "Keras has a module with image processing helper tools, located at `keras.preprocessing.image`. It contains the class `ImageDataGenerator` which allows to quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# rescale images by 1/255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    # target directory\n",
    "    train_dir,\n",
    "    # resizes all images to 150 x 150\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    # because we use binary_crossentropy loss, we need binary labels\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of one of these generators: it yields batches of 150x150 RGB images (shape `(20, 150, 150, 3)`) and binary labels (shape `(20,)`). 20 is the number of samples in each batch (the batch size). The generator yields these batches indefinitely: it just loops endlessly over the images present in the target folder. For this reason, we need to `break` the iteration loop at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit our model to the data using the generator we use the `fit_generator` method, the equivalent of `fit` for data generators. It expects as first argument a Python generator that will yield batches of inputs and targets indefinitely. \n",
    "Because the data is being generated endlessly, the generator needs to know how many samples to draw from the generator before declaring an epoch over. This is the role of the `steps_per_epoch` argument: after having drawn `steps_per_epoch` batches from the generator, i.e. after having run for `steps_per_epoch` gradient descent steps, the fitting process will go to the next epoch. In our case, batches are 20-sample large, so it will take 100 batches until we see our target of 2000 samples.\n",
    "\n",
    "When using `fit_generator`, one may pass a `validation_data` argument, This argument is allowed to be a data generator itself, but it could be a tuple of Numpy arrays as well. If you pass a generator as `validation_data`, then this generator is expected to yield batches of validation data endlessly, and thus you should also specify the `validation_steps` argument, which tells the process how many batches to draw from the validation generator for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0815 11:56:56.295122 139817182578496 deprecation_wrapper.py:119] From /home/mabeigier/anaconda3/envs/sklearn/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 60s 597ms/step - loss: 0.6899 - acc: 0.5345 - val_loss: 0.6620 - val_acc: 0.5920\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 0.6421 - acc: 0.6245 - val_loss: 0.6241 - val_acc: 0.6620\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 59s 592ms/step - loss: 0.5876 - acc: 0.6970 - val_loss: 0.6011 - val_acc: 0.6690\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 61s 607ms/step - loss: 0.5633 - acc: 0.7110 - val_loss: 0.5881 - val_acc: 0.6940\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 0.5360 - acc: 0.7175 - val_loss: 0.5979 - val_acc: 0.6770\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 60s 596ms/step - loss: 0.5126 - acc: 0.7415 - val_loss: 0.5904 - val_acc: 0.6800\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 0.4807 - acc: 0.7640 - val_loss: 0.6012 - val_acc: 0.6940\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 0.4630 - acc: 0.7855 - val_loss: 0.5941 - val_acc: 0.6860\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 61s 608ms/step - loss: 0.4376 - acc: 0.7960 - val_loss: 0.5500 - val_acc: 0.7130\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 60s 596ms/step - loss: 0.4232 - acc: 0.8085 - val_loss: 0.6692 - val_acc: 0.6900\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 65s 648ms/step - loss: 0.3932 - acc: 0.8250 - val_loss: 0.6269 - val_acc: 0.6780\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 63s 625ms/step - loss: 0.3693 - acc: 0.8325 - val_loss: 0.5575 - val_acc: 0.7130\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 97s 974ms/step - loss: 0.3451 - acc: 0.8530 - val_loss: 0.6088 - val_acc: 0.7140\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 62s 619ms/step - loss: 0.3134 - acc: 0.8750 - val_loss: 0.5736 - val_acc: 0.7330\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 60s 598ms/step - loss: 0.3018 - acc: 0.8740 - val_loss: 0.5733 - val_acc: 0.7250\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 60s 600ms/step - loss: 0.2718 - acc: 0.8900 - val_loss: 0.5973 - val_acc: 0.7140\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 60s 599ms/step - loss: 0.2573 - acc: 0.8985 - val_loss: 0.6033 - val_acc: 0.7110\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 62s 617ms/step - loss: 0.2298 - acc: 0.9140 - val_loss: 0.6520 - val_acc: 0.7430\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 60s 605ms/step - loss: 0.2213 - acc: 0.9140 - val_loss: 0.6517 - val_acc: 0.7380\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 59s 586ms/step - loss: 0.1987 - acc: 0.9195 - val_loss: 0.6709 - val_acc: 0.7310\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 58s 584ms/step - loss: 0.1844 - acc: 0.9375 - val_loss: 0.6513 - val_acc: 0.7410\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 59s 587ms/step - loss: 0.1604 - acc: 0.9445 - val_loss: 0.6623 - val_acc: 0.7480\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 60s 598ms/step - loss: 0.1457 - acc: 0.9520 - val_loss: 0.7102 - val_acc: 0.7530\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 58s 585ms/step - loss: 0.1267 - acc: 0.9595 - val_loss: 0.6893 - val_acc: 0.7560\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 59s 586ms/step - loss: 0.1246 - acc: 0.9550 - val_loss: 0.6924 - val_acc: 0.7420\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 59s 586ms/step - loss: 0.0980 - acc: 0.9675 - val_loss: 0.7523 - val_acc: 0.7340\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 59s 587ms/step - loss: 0.0896 - acc: 0.9720 - val_loss: 0.7681 - val_acc: 0.7520\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 60s 603ms/step - loss: 0.0787 - acc: 0.9730 - val_loss: 0.8323 - val_acc: 0.7410\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 59s 587ms/step - loss: 0.0678 - acc: 0.9790 - val_loss: 0.8269 - val_acc: 0.7400\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 59s 588ms/step - loss: 0.0540 - acc: 0.9870 - val_loss: 0.9330 - val_acc: 0.7300\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to always save your models after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs_small_1.5h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. \n",
    "Data augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same picture twice. This helps the model get exposed to more aspects of the data and generalize better.\n",
    "\n",
    "In Keras, this can be done by configuring a number of random transformations to be performed on the images read by our `ImageDataGenerator` instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `rotation_range` is a value in degrees (0-180), a range within which to randomly rotate pictures.\n",
    "* `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures \n",
    "vertically or horizontally.\n",
    "* `shear_range` is for randomly applying shearing transformations.\n",
    "* `zoom_range` is for randomly zooming inside pictures.\n",
    "* `horizontal_flip` is for randomly flipping half of the images horizontally -- relevant when there are no assumptions of horizontal \n",
    "asymmetry (e.g. real-world pictures).\n",
    "* `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module with image-preprocessing utilities\n",
    "from keras.preprocessing import image\n",
    "\n",
    "fnames = [os.path.join(train_cats_dir, fname) for\n",
    "         fname in os.listdir(train_cats_dir)]\n",
    "\n",
    "# chooses one image to augment\n",
    "img_path = fnames[3]\n",
    "\n",
    "# reads the image and resizes it\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "\n",
    "# converts it to Numpy array with shape (150, 150, 3)\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# reshapes it to (1, 150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# it will loop indefinitely, so we need to `break` the loop at some point\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train a new network using this data augmentation configuration, our network will never see twice the same input. However, the inputs that it sees are still heavily intercorrelated, since they come from a small number of original images. This might not be enough to completely get rid of overfitting. To further fight overfitting, we will also add a Dropout layer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                       input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network using data augmentation and dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # target directory\n",
    "        train_dir,\n",
    "        # all images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        # since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs_small_2.5h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c5343aa428ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
